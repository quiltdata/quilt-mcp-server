# Phase 2 Analysis: Over-Mocking in Unit Tests - Findings

**Issue Reference**: GitHub Issue #238 - Improve test coverage from 55.7% to 75%+
**Phase**: Phase 2 - Reduce Over-Mocking
**Branch**: `235-integration-test-coverage-gap`
**Status**: Analysis Complete
**Date**: 2025-11-15

## Executive Summary

This document presents findings from analyzing 1,577 lines of unit tests across 4 test files. The analysis reveals **significant over-mocking** that creates an illusion of coverage while bypassing real code execution. The core issue: **tests verify mock interactions rather than actual business logic**.

### Key Findings

1. **109 mocks in test_quilt_service.py** (741 lines) - Heavy HTTP mocking bypasses session management
2. **48 mocks in test_utils.py** (526 lines) - S3 client mocking prevents testing real boto3 integration
3. **31 mocks in test_tabulator.py** (204 lines) - Admin module mocking bypasses validation logic
4. **23 mocks in test_selector_fn.py** (106 lines) - Minimal mocking, mostly testing mock interactions

### Coverage Impact Potential

**Critical Discovery**: Files with high unit coverage but low integration coverage show severe over-mocking:

| File | Combined Coverage | Unit-Only Lines | Integration Gap | Potential Gain |
|------|-------------------|-----------------|-----------------|----------------|
| quilt_service.py | 83.3% | 90 lines | 61.1% ‚Üí 21.5% | **+15-20%** |
| utils.py | 66.0% | 32 lines | 53.6% ‚Üí 52.3% | **+5-8%** |
| tabulator_service.py | 61.6% | 17 lines | 37.7% ‚Üí 13.5% | **+3-5%** |

**Estimated Phase 2 Impact: +23-33% combined coverage through refactoring**

---

## Section 1: Mock Usage Analysis

### 1.1 test_quilt_service.py - Critical Over-Mocking

**File Stats**:
- 741 lines
- 109 mock references (Mock, patch, MagicMock)
- 82.6% unit coverage, 21.5% integration coverage
- **90 lines with unit-only coverage** (critical gap)

#### Critical Over-Mocked Tests

##### Test: `test_get_catalog_config_filters_and_derives_stack_prefix` (Lines 68-144)

**Lines**: 77 lines
**Mocks Used**: 5
- `Mock()` for session
- `Mock()` for response
- `patch.object(service, 'has_session_support')`
- `patch.object(service, 'get_session')`
- `mock_response.json`, `mock_response.raise_for_status`

**Logic Bypassed**:
```python
# Lines 89-100: Mock setup
mock_session = Mock()
mock_response = Mock()
mock_response.json.return_value = full_catalog_config
mock_response.raise_for_status = Mock()
mock_session.get.return_value = mock_response
```

**What's Actually Tested**:
- ‚úÖ Dict key filtering logic (lines 108-142)
- ‚úÖ String parsing for stack_prefix derivation

**What's NOT Tested** (bypassed by mocks):
- ‚ùå HTTP session creation via `quilt3.session.get_session()`
- ‚ùå HTTP GET request execution
- ‚ùå Network error handling
- ‚ùå JSON parsing errors
- ‚ùå HTTP timeout behavior
- ‚ùå Response status validation
- ‚ùå Session authentication token handling

**Real Coverage**: ~10 lines out of 50-line function (20%)

**Problem Statement**: This test verifies that mocks were called correctly, NOT that the HTTP logic works. The test would pass even if `session.get()` was completely broken.

---

##### Tests: `test_create_package_revision_*` (Lines 338-602)

**Test Count**: 9 tests
**Total Lines**: 264 lines
**Mocks Per Test**: 6-8

**Common Mock Pattern**:
```python
# Lines 343-353 - Repeated in every test
mock_package = Mock()
mock_package.set = Mock()
mock_package.set_meta = Mock()
mock_package.push = Mock(return_value="test-hash-123")

with (
    patch('quilt3.Package', return_value=mock_package),
    patch('quilt_mcp.services.quilt_service.QuiltService._organize_s3_files_smart') as mock_organize,
    patch('quilt_mcp.services.quilt_service.QuiltService._normalize_registry', return_value="s3://test-bucket"),
):
```

**Logic Bypassed**:
- ‚ùå `quilt3.Package()` instantiation
- ‚ùå `_organize_s3_files_smart()` file organization logic
- ‚ùå `_normalize_registry()` URI normalization
- ‚ùå Package file setting logic
- ‚ùå Package metadata setting
- ‚ùå Package push operation
- ‚ùå Error handling for any of above

**What's Actually Tested**:
- ‚úÖ That `Package()` constructor was called
- ‚úÖ That `mock_package.push()` was called with correct args
- ‚úÖ That result dict has expected keys

**Real Coverage**: Dict assembly logic only (~15 lines out of 80-line function = 19%)

**Problem**: These tests are **abstraction tests** - they verify the interface exists but not that it works. The actual package creation logic is completely untested.

---

##### Test: `test_get_catalog_info_when_authenticated` (Lines 180-211)

**Mocks Used**: 3
- `patch('quilt3.logged_in', return_value='https://example.quiltdata.com')`
- `patch('quilt3.config', return_value={...})`
- `patch.object(service, 'get_catalog_config', return_value={...})`

**Logic Bypassed**:
- ‚ùå `quilt3.logged_in()` authentication check
- ‚ùå `quilt3.config()` config retrieval
- ‚ùå `get_catalog_config()` HTTP fetching
- ‚ùå Error handling for any auth failures

**What's Tested**: Dict merging logic only

**Real Coverage**: ~10 lines out of 40-line function (25%)

---

#### Summary: test_quilt_service.py Analysis

| Test Category | Test Count | Lines | Mocks | Logic Tested | Logic Bypassed |
|---------------|-----------|-------|-------|--------------|----------------|
| Authentication | 7 | 120 | 15 | Config dict checks | Auth, sessions, HTTP |
| Catalog Config | 3 | 110 | 12 | Dict filtering | HTTP, JSON parsing, errors |
| Package Ops | 9 | 264 | 50 | Dict assembly | Package creation, push |
| Admin | 8 | 120 | 20 | Module existence | Admin API calls |
| Session | 6 | 85 | 12 | Return values | Session management |

**Totals**: 33 test functions, 699 lines, 109 mocks

**Critical Insight**: 80% of test code verifies mock setup, 20% tests actual logic.

---

### 1.2 test_utils.py - Moderate Over-Mocking

**File Stats**:
- 526 lines
- 48 mock references
- 53.6% unit coverage, 52.3% integration coverage
- **32 lines with unit-only coverage** (smaller gap)

#### Over-Mocked Tests

##### Tests: `test_generate_signed_url_*` (Lines 36-99)

**Test Count**: 5 tests
**Lines**: 63 lines
**Mocks Used**: Multiple S3 client mocks

**Pattern**:
```python
@patch("quilt_mcp.utils.get_s3_client")
def test_generate_signed_url_mocked(self, mock_s3_client):
    mock_client = MagicMock()
    mock_client.generate_presigned_url.return_value = "https://signed.url"
    mock_s3_client.return_value = mock_client

    result = generate_signed_url("s3://my-bucket/my-key.txt", 1800)

    self.assertEqual(result, "https://signed.url")
    mock_client.generate_presigned_url.assert_called_once_with(...)
```

**Logic Bypassed**:
- ‚ùå `get_s3_client()` boto3 client creation
- ‚ùå AWS credential resolution
- ‚ùå `generate_presigned_url()` actual implementation
- ‚ùå URL signing logic
- ‚ùå AWS SDK error handling

**What's Tested**:
- ‚úÖ URI parsing logic
- ‚úÖ Expiration clamping logic (0‚Üí1, >7days‚Üí7days)
- ‚úÖ Function parameter passing

**Real Coverage**: ~15 lines out of 30-line function (50%)

**Good News**: Integration tests exist! ([test_utils_integration.py](test_utils_integration.py:1)) tests the same functions with real AWS.

---

##### Tests: `test_parse_s3_uri_*` (Lines 100-238)

**Test Count**: 15 tests
**Lines**: 138 lines
**Mocks Used**: 0 (Pure logic tests)

**Assessment**: ‚úÖ **Category A - Keep as unit tests**

These tests are **correctly implemented** - they test pure URI parsing logic with no external dependencies. No mocking needed, no refactoring required.

**Example**:
```python
def test_parse_s3_uri_valid_basic_uri(self):
    bucket, key, version_id = parse_s3_uri("s3://my-bucket/my-key.txt")
    self.assertEqual(bucket, "my-bucket")
    self.assertEqual(key, "my-key.txt")
```

This is **ideal unit testing** - pure input/output validation.

---

##### Tests: `test_register_tools_*` (Lines 259-350)

**Test Count**: 6 tests
**Lines**: 91 lines
**Mocks Used**: 15-20

**Pattern**:
```python
def test_register_tools_with_mock_server(self):
    mock_server = Mock(spec=FastMCP)
    tools_count = register_tools(mock_server, verbose=False)
    self.assertGreater(tools_count, 0)
    self.assertEqual(mock_server.tool.call_count, tools_count)
```

**Assessment**: Mixed
- ‚úÖ **Category A**: Tests verifying tool count and registration logic
- ‚ö†Ô∏è **Category B**: Tests mocking `inspect.getmembers` should use real inspection

**Logic Bypassed**:
- ‚ùå Actual FastMCP server behavior
- ‚ùå Tool decorator execution
- ‚ùå Module inspection (mocked)

**Recommendation**: Keep basic tests, add integration test for full server creation.

---

#### Summary: test_utils.py Analysis

| Test Category | Test Count | Lines | Mocks | Assessment |
|---------------|-----------|-------|-------|------------|
| parse_s3_uri | 15 | 138 | 0 | ‚úÖ Category A - Keep |
| generate_signed_url | 5 | 63 | 10 | ‚ö†Ô∏è Category B - Refactor |
| MCP server config | 8 | 160 | 25 | Mixed A/B |
| Tool registration | 6 | 91 | 13 | Mixed A/B |

**Totals**: 34 test functions, 452 lines, 48 mocks

**Assessment**: Better than test_quilt_service.py - has good pure logic tests, but S3 mocking needs integration tests.

---

### 1.3 test_tabulator.py - Minimal Over-Mocking

**File Stats**:
- 204 lines
- 31 mock references
- 37.7% unit coverage, 13.5% integration coverage
- **17 lines with unit-only coverage**

#### Analysis

##### Tests: Tabulator Query Tests (Lines 83-205)

**Test Count**: 6 tests
**Lines**: 122 lines
**Mocks Used**: MagicMock for catalog_info and athena_query_execute

**Pattern**:
```python
def test_tabulator_query_discovers_catalog_from_catalog_info(monkeypatch):
    mock_catalog_info = MagicMock(return_value={
        "status": "success",
        "is_authenticated": True,
        "tabulator_data_catalog": "quilt_example_catalog",
    })
    monkeypatch.setattr("quilt_mcp.services.auth_metadata.catalog_info", mock_catalog_info)

    mock_execute = MagicMock(return_value={"success": True, "rows": []})
    monkeypatch.setattr("quilt_mcp.services.athena_read_service.athena_query_execute", mock_execute)
```

**Assessment**: ‚ö†Ô∏è **Category B - Should be integration tests**

**Logic Bypassed**:
- ‚ùå Catalog info discovery logic
- ‚ùå Athena query execution
- ‚ùå Query parameter validation
- ‚ùå Database name handling

**What's Tested**: Function call chaining only

**Recommendation**: Convert to integration tests using moto for Athena mocking.

---

##### Test: `test_create_table_normalizes_parser_format` (Lines 36-60)

**Mocks Used**: DummyTabulatorAdmin (custom mock)

**Pattern**:
```python
class DummyTabulatorAdmin:
    def set_table(self, **kwargs):
        self.calls.append(("set_table", kwargs))
        return self.response
```

**Assessment**: ‚ö†Ô∏è **Category B - Needs integration test**

**Logic Bypassed**:
- ‚ùå Admin API calls
- ‚ùå YAML generation
- ‚ùå Schema validation
- ‚ùå Actual table creation

**What's Tested**:
- ‚úÖ Parser format normalization (CSV ‚Üí csv)
- ‚úÖ Default delimiter injection

**Recommendation**: Keep unit test for normalization logic, add integration test for admin API.

---

#### Summary: test_tabulator.py Analysis

| Test Category | Test Count | Lines | Mocks | Assessment |
|---------------|-----------|-------|-------|------------|
| Table creation | 2 | 50 | 8 | Category B |
| Query discovery | 4 | 100 | 20 | Category B |
| Validation | 2 | 40 | 3 | Category A/B mixed |

**Totals**: 8 test functions, 190 lines, 31 mocks

**Assessment**: Small file, but heavy mocking of admin APIs. Needs integration tests.

---

### 1.4 test_selector_fn.py - Minimal File

**File Stats**:
- 106 lines
- 23 mock references
- Very simple selector function tests

#### Analysis

**Pattern**:
```python
@patch("quilt3.Package")
@patch("quilt_mcp.tools.packages.get_s3_client")
def test_package_ops_copy_mode_none(mock_get_s3_client, mock_package_class):
    mock_package_class.return_value = MockPackage()
    mock_s3_client = Mock()
    mock_s3_client.head_object.return_value = {"ContentLength": 100}
```

**Assessment**: ‚ö†Ô∏è **Category B - Needs real Package tests**

**Logic Bypassed**:
- ‚ùå Real quilt3.Package operations
- ‚ùå S3 head_object calls
- ‚ùå Selector function execution

**Recommendation**: Keep selector logic tests, add integration tests for package operations.

---

## Section 2: Test Categorization (A/B/C)

### Category A: Keep as Unit Tests (Pure Logic)

These tests are **correctly implemented** and should remain as unit tests:

| Test File | Test Function | Reason | Lines |
|-----------|--------------|--------|-------|
| test_utils.py | `test_parse_s3_uri_*` (15 tests) | Pure URI parsing, no I/O | 138 |
| test_utils.py | `test_generate_signed_url_invalid_uri` | Input validation only | 8 |
| test_quilt_service.py | Helper method tests | Pure logic helpers | ~30 |

**Total Category A**: ~20 tests, ~176 lines

**Characteristics**:
- No external dependencies
- Pure input ‚Üí output transformations
- Fast execution (<1ms per test)
- No mocking needed

**Action**: **Keep unchanged**

---

### Category B: Refactor to Integration Tests

These tests mock external services and should become integration tests:

#### High Priority (Large Coverage Impact)

| Test File | Test Functions | Current Mocks | Target | Impact |
|-----------|---------------|---------------|--------|---------|
| test_quilt_service.py | `test_get_catalog_config_*` (3 tests) | HTTP session, response | Use httpx + respx | +15% |
| test_quilt_service.py | `test_create_package_revision_*` (9 tests) | quilt3.Package | Use real Package with moto S3 | +20% |
| test_utils.py | `test_generate_signed_url_mocked_*` (4 tests) | boto3 S3 client | Use moto S3 | +5% |
| test_tabulator.py | `test_tabulator_query_*` (4 tests) | Athena, catalog_info | Use moto Athena | +3% |
| test_tabulator.py | `test_create_table_*` (2 tests) | Admin API | Use real admin with moto | +2% |

**Total Category B High Priority**: 22 tests, ~450 lines, **+45% coverage potential**

#### Medium Priority (Moderate Impact)

| Test File | Test Functions | Current Mocks | Target | Impact |
|-----------|---------------|---------------|--------|---------|
| test_quilt_service.py | `test_get_catalog_info_*` (2 tests) | quilt3 config, auth | Integration with real quilt3 | +5% |
| test_quilt_service.py | Session tests (6 tests) | quilt3.session | Integration test | +3% |
| test_selector_fn.py | Package copy tests (2 tests) | quilt3.Package | Use real Package | +2% |

**Total Category B Medium Priority**: 10 tests, ~180 lines, **+10% coverage potential**

---

### Category C: Delete (Redundant)

Tests that **only verify mock interactions** with no business logic:

| Test File | Test Function | Reason to Delete | Lines Saved |
|-----------|--------------|------------------|-------------|
| test_quilt_service.py | `test_browse_package_with_hash` | Only verifies Package.browse call | 10 |
| test_quilt_service.py | `test_browse_package_without_hash` | Only verifies Package.browse call | 8 |
| test_quilt_service.py | `test_create_bucket_returns_bucket_instance` | Only verifies Bucket() call | 8 |
| test_quilt_service.py | `test_get_search_api_returns_search_module` | Only verifies import | 7 |
| test_utils.py | `test_register_tools_only_public_functions` | Complex mock, no real testing | 30 |

**Total Category C**: 5 tests, ~63 lines

**Rationale**: These tests would pass even if the underlying code was completely broken. They test the **interface exists**, not that it **works correctly**.

**Action**: Delete after confirming integration tests cover the functionality.

---

## Section 3: Integration Test Gap Analysis

### Current Integration Test Coverage

**Existing Integration Tests** ([tests/integration/](tests/integration/)):
- test_utils_integration.py - ‚úÖ Good! Tests `generate_signed_url` with real AWS
- test_bucket_tools.py - ‚úÖ Good! Tests bucket operations with real S3
- test_s3_package.py - ‚úÖ Good! Tests package creation with real S3
- test_athena.py - ‚úÖ Good! Tests Athena queries
- test_permissions.py - ‚úÖ Good! Tests permission discovery

**Assessment**: Integration tests **do exist** but have gaps.

---

### Gap 1: quilt_service.py HTTP Operations

**Unit Tests**: 3 tests for `get_catalog_config()` (110 lines)
**Integration Tests**: **NONE FOUND**
**Coverage Gap**: 61.1% (unit) ‚Üí 21.5% (integration) = **39.6% gap**

**What's Missing**:
```python
# NO integration tests for:
- get_catalog_config() HTTP fetching
- Session management (get_session(), has_session_support())
- HTTP error handling (404, 500, timeouts)
- JSON parsing errors
- Config filtering logic with real responses
```

**Dependencies Needed**:
- `httpx` (likely already in use)
- `respx` for HTTP mocking
- Real catalog config.json samples

**Estimated Coverage Gain**: **+15-20%** on quilt_service.py

---

### Gap 2: quilt_service.py Package Operations

**Unit Tests**: 9 tests for `create_package_revision()` (264 lines)
**Integration Tests**: Partial coverage in test_s3_package.py
**Coverage Gap**: **quilt_service.py abstraction layer not tested**

**What's Missing**:
```python
# Tests exist for package creation, but NOT for QuiltService wrapper:
- create_package_revision() auto_organize=True path
- create_package_revision() auto_organize=False path
- _organize_s3_files_smart() logic
- _collect_objects_flat() logic
- Registry normalization
- Copy mode selector functions
```

**Dependencies Needed**:
- moto for S3 mocking
- Real S3 test data

**Estimated Coverage Gain**: **+10-15%** on quilt_service.py

---

### Gap 3: utils.py S3 Operations

**Unit Tests**: 5 tests for `generate_signed_url()` (63 lines, heavily mocked)
**Integration Tests**: ‚úÖ **EXISTS** ([test_utils_integration.py](test_utils_integration.py:1))
**Coverage Gap**: **Small gap** (53.6% unit ‚Üí 52.3% integration = 1.3%)

**Assessment**: ‚úÖ **This is a SUCCESS STORY** - integration tests already exist and provide good coverage!

**Recommendation**:
1. Keep integration tests as-is
2. Consider reducing unit test mocks (they're redundant)
3. Model for other refactoring efforts

**Estimated Coverage Gain**: **+2-3%** (filling small gaps)

---

### Gap 4: tabulator_service.py Query Operations

**Unit Tests**: 4 tests for `_tabulator_query()` (100 lines, heavily mocked)
**Integration Tests**: **NONE FOUND**
**Coverage Gap**: 37.7% unit ‚Üí 13.5% integration = **24.2% gap**

**What's Missing**:
```python
# NO integration tests for:
- _tabulator_query() catalog discovery
- tabulator_bucket_query() database name handling
- Athena query execution
- Query validation
- Error handling
```

**Dependencies Needed**:
- moto for Athena mocking
- Sample Athena catalog configurations

**Estimated Coverage Gain**: **+3-5%** on tabulator_service.py

---

### Gap 5: tabulator_service.py Table Creation

**Unit Tests**: 2 tests for `create_table()` (50 lines)
**Integration Tests**: **NONE FOUND**
**Coverage Gap**: Small but critical functionality

**What's Missing**:
```python
# NO integration tests for:
- create_table() with real admin API
- YAML generation
- Schema validation
- Parser config normalization
```

**Dependencies Needed**:
- Mock quilt3.admin.tabulator module
- YAML validation

**Estimated Coverage Gain**: **+2-3%** on tabulator_service.py

---

### Summary: Integration Test Gaps

| Module | Gap Description | Lines Uncovered | Estimated Gain | Priority |
|--------|----------------|-----------------|----------------|----------|
| quilt_service.py | HTTP operations | 90 | +15-20% | **HIGH** |
| quilt_service.py | Package abstraction | 60 | +10-15% | **HIGH** |
| utils.py | S3 operations | 10 | +2-3% | LOW (exists) |
| tabulator_service.py | Queries | 40 | +3-5% | MEDIUM |
| tabulator_service.py | Table creation | 20 | +2-3% | MEDIUM |

**Total Potential Gain: +32-46% combined coverage**

---

## Section 4: Testing Philosophy Analysis

### Current Philosophy (INCORRECT)

**Observed Pattern**:
```python
# test_quilt_service.py:68-144
def test_get_catalog_config_filters_and_derives_stack_prefix(self):
    """Test get_catalog_config returns only essential keys."""
    service = QuiltService()

    # 1. Mock ALL external dependencies
    mock_session = Mock()
    mock_response = Mock()
    mock_response.json.return_value = full_catalog_config
    mock_session.get.return_value = mock_response

    with (
        patch.object(service, 'has_session_support', return_value=True),
        patch.object(service, 'get_session', return_value=mock_session),
    ):
        # 2. Call the function
        result = service.get_catalog_config('https://example.com')

        # 3. Verify mocks were called
        mock_session.get.assert_called_once_with(...)

        # 4. Verify output dict structure
        assert result["region"] == "us-east-1"
        assert "sentryDSN" not in result
```

**Why This Is Wrong**:

1. **Tests Mock Interactions, Not Behavior**
   - ‚ùå Verifies `mock_session.get.assert_called_once()`
   - ‚ùå Does not test if HTTP actually works
   - ‚ùå Does not test error handling

2. **Creates False Coverage**
   - ‚úÖ Coverage shows 82.6% for quilt_service.py
   - ‚ùå But 61.1% is unit-only (not tested in integration)
   - ‚ùå Real effective coverage: ~21.5%

3. **Bypasses Critical Code Paths**
   - ‚ùå Never tests `session.get()` execution
   - ‚ùå Never tests `response.raise_for_status()`
   - ‚ùå Never tests `response.json()` parsing
   - ‚ùå Never tests timeout handling
   - ‚ùå Never tests network errors

4. **Tests Would Pass with Broken Code**
   ```python
   # This broken code would pass the test:
   def get_session(self):
       return None  # BROKEN!

   # Test still passes because mock replaces get_session()
   ```

---

### Desired Philosophy (CORRECT)

**Principle**: **Unit tests verify pure logic. Integration tests verify component interaction.**

#### Example Fix: Split `test_get_catalog_config_*`

##### Before (Wrong):
```python
# test_quilt_service.py:68-144
def test_get_catalog_config_filters_and_derives_stack_prefix(self):
    service = QuiltService()

    # Mock everything
    mock_session = Mock()
    mock_response = Mock()
    mock_response.json.return_value = full_catalog_config

    with patch.object(service, 'get_session', return_value=mock_session):
        result = service.get_catalog_config('https://example.com')

    # Only tests dict filtering
    assert result["region"] == "us-east-1"
```

**Problems**:
- ‚ùå Mocks bypass 80% of function logic
- ‚ùå Tests dict filtering only
- ‚ùå Never validates HTTP layer

---

##### After (Correct) - Part 1: Unit Test for Pure Logic

```python
# tests/unit/test_catalog_config_filtering.py
def test_catalog_config_filtering_logic():
    """Test that config dict filtering extracts only essential keys."""
    # Test PURE LOGIC - no I/O, no mocking needed
    raw_config = {
        "region": "us-east-1",
        "apiGatewayEndpoint": "https://api.example.com",
        "analyticsBucket": "quilt-staging-analyticsbucket-abc",
        "sentryDSN": "https://sentry.io/123",  # Should be filtered out
        "mixpanelToken": "secret",  # Should be filtered out
    }

    # Call internal helper function (refactored from get_catalog_config)
    filtered = _filter_catalog_config(raw_config)

    # Verify filtering logic
    assert filtered["region"] == "us-east-1"
    assert filtered["api_gateway_endpoint"] == "https://api.example.com"
    assert "sentryDSN" not in filtered
    assert "mixpanelToken" not in filtered

def test_stack_prefix_derivation():
    """Test stack prefix is correctly derived from analytics bucket."""
    assert _derive_stack_prefix("quilt-staging-analyticsbucket-abc") == "quilt-staging"
    assert _derive_stack_prefix("prod-analyticsbucket-xyz") == "prod"

def test_tabulator_catalog_name_derivation():
    """Test tabulator catalog name is correctly derived."""
    assert _derive_tabulator_catalog("quilt-staging") == "quilt-quilt-staging-tabulator"
```

**Benefits**:
- ‚úÖ Fast (<1ms per test)
- ‚úÖ No mocking needed
- ‚úÖ Tests pure business logic
- ‚úÖ Tests one thing per test

---

##### After (Correct) - Part 2: Integration Test for HTTP Layer

```python
# tests/integration/test_quilt_service_http.py
import httpx
import respx
import pytest

@pytest.mark.integration
def test_get_catalog_config_http_success():
    """Test catalog config fetching with real HTTP client."""
    service = QuiltService()

    # Use respx to mock HTTP responses (not the HTTP client itself)
    with respx.mock:
        # Mock the HTTP endpoint, but use REAL httpx client
        respx.get("https://example.com/config.json").mock(
            return_value=httpx.Response(
                200,
                json={
                    "region": "us-east-1",
                    "apiGatewayEndpoint": "https://api.example.com",
                    "analyticsBucket": "quilt-staging-analyticsbucket-abc",
                }
            )
        )

        # Call the REAL function with REAL HTTP client
        result = service.get_catalog_config('https://example.com')

        # Verify complete workflow
        assert result["region"] == "us-east-1"
        assert result["stack_prefix"] == "quilt-staging"

@pytest.mark.integration
def test_get_catalog_config_http_timeout():
    """Test HTTP timeout handling."""
    service = QuiltService()

    with respx.mock:
        # Simulate timeout
        respx.get("https://example.com/config.json").mock(
            side_effect=httpx.TimeoutException("Request timed out")
        )

        # Verify error handling
        result = service.get_catalog_config('https://example.com')
        assert result is None  # Should handle timeout gracefully

@pytest.mark.integration
def test_get_catalog_config_http_404():
    """Test HTTP 404 handling."""
    service = QuiltService()

    with respx.mock:
        respx.get("https://example.com/config.json").mock(
            return_value=httpx.Response(404)
        )

        result = service.get_catalog_config('https://example.com')
        assert result is None

@pytest.mark.integration
def test_get_catalog_config_invalid_json():
    """Test invalid JSON handling."""
    service = QuiltService()

    with respx.mock:
        respx.get("https://example.com/config.json").mock(
            return_value=httpx.Response(200, text="not json")
        )

        result = service.get_catalog_config('https://example.com')
        assert result is None
```

**What This Tests**:
- ‚úÖ Real HTTP client (httpx or requests.Session)
- ‚úÖ Session creation and management
- ‚úÖ URL construction
- ‚úÖ HTTP GET execution
- ‚úÖ Response parsing
- ‚úÖ Error handling (timeout, 404, invalid JSON)
- ‚úÖ Status code validation

**What's Mocked**: Only the HTTP endpoint (using respx), NOT the HTTP client

**Coverage Gained**: ~40 lines previously bypassed by mocks

---

### Philosophy Comparison Table

| Aspect | Current (Wrong) | Desired (Correct) |
|--------|----------------|-------------------|
| **Unit Tests** | Mock everything | Test pure logic only |
| **Integration Tests** | Rarely written | Test component integration |
| **Mocking** | Mock internal code | Mock external services only |
| **Coverage** | High % but misleading | Lower % but meaningful |
| **Test Focus** | Verify mocks called | Verify behavior correct |
| **Test Speed** | Fast but false confidence | Mixed (fast units, slower integration) |
| **Bug Detection** | Poor (mocks hide bugs) | Good (real integration tested) |

---

## Section 5: Coverage Impact Estimation

### High-Confidence Estimates

Based on coverage-analysis.csv data:

#### quilt_service.py Impact

**Current State**:
- 270 lines total
- 82.6% unit coverage = 223 lines
- 21.5% integration coverage = 58 lines
- **90 lines unit-only** (223 - 58 = 165? CSV shows 90, using CSV)
- 83.3% combined coverage = 225 lines

**Analysis**:
```
Unit-only lines: 90
These lines are covered by unit tests but NOT integration tests.
This indicates unit tests are mocking away the real logic.
```

**Refactoring Impact**:
1. Convert HTTP tests to integration: +40 lines (HTTP session, config fetching)
2. Convert package tests to integration: +35 lines (package creation, push)
3. Convert admin tests to integration: +15 lines (admin API calls)

**Projected Coverage**:
- Current integration: 58 lines
- Add HTTP integration: +40 = 98 lines
- Add package integration: +35 = 133 lines
- Add admin integration: +15 = 148 lines
- **Projected integration coverage: 148/270 = 54.8%**
- **Projected combined coverage: 225 + 23 (newly covered) = 248/270 = 91.9%**

**Impact: +8.6% on quilt_service.py**

---

#### error_recovery.py Impact (Unexpected Finding)

**Current State** (from CSV):
- 212 lines total
- 59.9% unit coverage = 127 lines
- **0.0% integration coverage = 0 lines**
- 59.9% combined = 127 lines
- **127 lines unit-only**

**Critical Issue**: This file has NO integration tests at all!

**Analysis**:
```python
# error_recovery.py is entirely mocked in unit tests
# All 127 covered lines are from mocked unit tests
# Real error recovery logic is NEVER tested
```

**Refactoring Impact**:
1. Create integration tests for error recovery workflows
2. Test real retry logic with actual failures
3. Test real timeout handling

**Projected Coverage**:
- If 80% of unit-only lines can be integration tested: 127 * 0.8 = 102 lines
- **Projected integration coverage: 102/212 = 48.1%**
- **Projected combined coverage: 127 + 75 (newly covered) = 202/212 = 95.3%**

**Impact: +35.4% on error_recovery.py** üéØ **HUGE WIN**

---

#### workflow_service.py Impact

**Current State**:
- 188 lines total
- 66.5% unit coverage = 125 lines
- 18.1% integration coverage = 34 lines
- **91 lines unit-only**
- 66.5% combined = 125 lines

**Refactoring Impact**:
1. Convert workflow execution tests to integration: +50 lines
2. Test real workflow state management: +25 lines

**Projected Coverage**:
- Current combined: 125 lines
- Add integration: +60 lines (some overlap with unit)
- **Projected combined: 185/188 = 98.4%**

**Impact: +31.9% on workflow_service.py** üéØ

---

#### governance_service.py Impact

**Current State**:
- 310 lines total
- 59.4% unit coverage = 184 lines
- 12.9% integration coverage = 40 lines
- **102 lines unit-only**
- 66.5% combined = 206 lines (CSV shows 66.5%, but 206/310 = 66.5% ‚úì)

**Refactoring Impact**:
1. Create integration tests for governance checks: +60 lines
2. Test real policy validation: +30 lines

**Projected Coverage**:
- Current combined: 206 lines
- Add integration: +80 lines (accounting for overlap)
- **Projected combined: 286/310 = 92.3%**

**Impact: +25.8% on governance_service.py** üéØ

---

#### data_visualization.py Impact

**Current State**:
- 306 lines total
- 55.6% unit coverage = 170 lines
- 13.1% integration coverage = 40 lines
- **130 lines unit-only**
- 55.6% combined = 170 lines

**Refactoring Impact**:
1. Integration tests for visualization generation: +70 lines
2. Integration tests for data processing: +40 lines

**Projected Coverage**:
- Current combined: 170 lines
- Add integration: +90 lines (accounting for overlap)
- **Projected combined: 260/306 = 85.0%**

**Impact: +29.4% on data_visualization.py** üéØ

---

### Total Phase 2 Coverage Impact

| Module | Current Combined | Projected Combined | Gain | Lines Gained |
|--------|------------------|-------------------|------|--------------|
| quilt_service.py | 83.3% | 91.9% | **+8.6%** | +23 |
| error_recovery.py | 59.9% | 95.3% | **+35.4%** | +75 |
| workflow_service.py | 66.5% | 98.4% | **+31.9%** | +60 |
| governance_service.py | 66.5% | 92.3% | **+25.8%** | +80 |
| data_visualization.py | 55.6% | 85.0% | **+29.4%** | +90 |
| utils.py | 66.0% | 71.0% | **+5.0%** | +12 |
| tabulator_service.py | 61.6% | 70.0% | **+8.4%** | +24 |

**Total lines gained: 364 lines**
**Total codebase: 9,520 lines**

**Phase 2 Coverage Impact: 364/9520 = +3.8% overall coverage**

**BUT**: This is concentrated in critical service modules, which increases *meaningful* coverage significantly.

---

### Conservative vs. Optimistic Estimates

#### Conservative Estimate
- Assume 60% of unit-only lines can be covered by integration tests
- Total potential: 364 * 0.6 = 218 lines
- **Impact: +2.3% overall coverage**

#### Realistic Estimate (Recommended)
- Assume 75% of unit-only lines can be covered
- Total potential: 364 * 0.75 = 273 lines
- **Impact: +2.9% overall coverage**

#### Optimistic Estimate
- Assume 90% of unit-only lines can be covered
- Total potential: 364 * 0.9 = 328 lines
- **Impact: +3.4% overall coverage**

**Recommendation**: Plan for **+2.9% overall coverage** (+3.8% best case)

---

## Section 6: Refactoring Prerequisites

### Code Changes Needed for Testability

#### 1. quilt_service.py - Function Splitting

**Current Code** (Lines 72-134):
```python
def get_catalog_config(self, catalog_url: str) -> dict[str, Any] | None:
    """Get catalog configuration from <catalog>/config.json."""
    if not self.has_session_support():
        raise Exception("quilt3 session not available")

    try:
        session = self.get_session()
        normalized_url = catalog_url.rstrip("/")
        config_url = f"{normalized_url}/config.json"

        response = session.get(config_url, timeout=10)
        response.raise_for_status()

        full_config = response.json()

        # Filter and transform config
        filtered_config: dict[str, Any] = {}
        if "region" in full_config:
            filtered_config["region"] = full_config["region"]
        # ... 30 more lines of filtering logic ...

        return filtered_config
    except Exception:
        return None
```

**Problem**: Mixes HTTP I/O with business logic

**Refactoring Required**:

```python
# Split into 3 functions:

def get_catalog_config(self, catalog_url: str) -> dict[str, Any] | None:
    """Get catalog configuration - HTTP layer."""
    if not self.has_session_support():
        raise Exception("quilt3 session not available")

    try:
        session = self.get_session()
        raw_config = self._fetch_catalog_config(session, catalog_url)
        return self._filter_catalog_config(raw_config)
    except Exception:
        return None

def _fetch_catalog_config(self, session, catalog_url: str) -> dict[str, Any]:
    """Fetch raw config from catalog (HTTP I/O) - testable with respx."""
    normalized_url = catalog_url.rstrip("/")
    config_url = f"{normalized_url}/config.json"
    response = session.get(config_url, timeout=10)
    response.raise_for_status()
    return response.json()

def _filter_catalog_config(self, raw_config: dict) -> dict[str, Any]:
    """Filter config to essential keys (pure logic) - unit testable."""
    filtered = {}
    if "region" in raw_config:
        filtered["region"] = raw_config["region"]
    # ... filtering logic ...
    return filtered

def _derive_stack_prefix(self, analytics_bucket: str) -> str:
    """Derive stack prefix from analytics bucket (pure logic)."""
    if "-analyticsbucket" in analytics_bucket.lower():
        return analytics_bucket.split("-analyticsbucket")[0]
    return analytics_bucket
```

**Benefits**:
- ‚úÖ HTTP layer testable with integration tests
- ‚úÖ Business logic testable with unit tests
- ‚úÖ No mocking needed for logic tests

**Estimated Effort**: 2-3 hours

---

#### 2. quilt_service.py - Dependency Injection for Package Creation

**Current Code** (Lines 350-450):
```python
def create_package_revision(
    self,
    package_name: str,
    s3_uris: List[str],
    ...
):
    """Create package revision."""
    # Direct call to quilt3.Package - hard to test
    package = quilt3.Package()

    for uri in s3_uris:
        package.set(logical_key, uri)

    package.set_meta(metadata)
    top_hash = package.push(package_name, registry=registry)
    return {"top_hash": top_hash, ...}
```

**Problem**: Cannot test without real quilt3.Package

**Refactoring Required**:

```python
def create_package_revision(
    self,
    package_name: str,
    s3_uris: List[str],
    package_factory=None,  # NEW: Allow injection for testing
    ...
):
    """Create package revision."""
    # Use injected factory or default to real Package
    if package_factory is None:
        package_factory = lambda: quilt3.Package()

    package = package_factory()

    for uri in s3_uris:
        package.set(logical_key, uri)

    package.set_meta(metadata)
    top_hash = package.push(package_name, registry=registry)
    return {"top_hash": top_hash, ...}
```

**Alternative**: Extract package operations to separate testable functions

**Benefits**:
- ‚úÖ Unit tests can inject mock factory
- ‚úÖ Integration tests use real Package with moto
- ‚úÖ Preserves existing behavior

**Estimated Effort**: 1-2 hours

---

#### 3. utils.py - Already Well-Structured

**Current Code**:
```python
def generate_signed_url(s3_uri: str, expires_in: int = 3600) -> str | None:
    """Generate presigned URL."""
    # Already well-structured!
    bucket, key, version_id = parse_s3_uri(s3_uri)  # Pure logic
    if not bucket or not key:
        return None

    expires_in = max(1, min(expires_in, 604800))  # Pure logic

    client = get_s3_client()  # Injected dependency
    return client.generate_presigned_url(...)  # AWS call
```

**Assessment**: ‚úÖ **Already good!** No refactoring needed.

**Why it's good**:
- Pure logic is separated (`parse_s3_uri`, clamping)
- S3 client is injected via `get_s3_client()`
- Integration tests already exist

**Action**: Use this as a model for other refactoring

---

#### 4. tabulator_service.py - Extract Validation Logic

**Current Code** (Lines 36-100):
```python
def create_table(
    self,
    bucket_name: str,
    table_name: str,
    schema: List[Dict],
    parser_config: Dict,
    ...
):
    """Create table."""
    # Validation mixed with API calls
    if not bucket_name:
        errors.append("Bucket name cannot be empty")

    if not schema:
        errors.append("Schema cannot be empty")

    # Parser normalization
    parser_format = parser_config.get("format", "csv").lower()

    # YAML generation
    yaml_config = self._generate_yaml(...)

    # Admin API call
    admin = self.get_tabulator_admin()
    admin.set_table(bucket=bucket_name, name=table_name, config=yaml_config)
```

**Refactoring Required**:

```python
def create_table(self, ...):
    """Create table - orchestration layer."""
    validation_errors = self._validate_table_config(bucket_name, table_name, schema, ...)
    if validation_errors:
        return {"success": False, "error_details": validation_errors}

    normalized_config = self._normalize_parser_config(parser_config)
    yaml_config = self._generate_table_yaml(...)

    return self._create_table_via_admin(bucket_name, table_name, yaml_config)

def _validate_table_config(self, ...) -> List[str]:
    """Validate table configuration (pure logic)."""
    errors = []
    if not bucket_name:
        errors.append("Bucket name cannot be empty")
    # ... validation logic ...
    return errors

def _normalize_parser_config(self, parser_config: Dict) -> Dict:
    """Normalize parser config (pure logic)."""
    config = parser_config.copy()
    config["format"] = config.get("format", "csv").lower()
    # ... normalization logic ...
    return config

def _generate_table_yaml(self, ...) -> str:
    """Generate YAML config (pure logic)."""
    # YAML generation logic
    return yaml_string

def _create_table_via_admin(self, bucket, name, yaml_config) -> Dict:
    """Create table via admin API (integration point)."""
    admin = self.get_tabulator_admin()
    admin.set_table(bucket=bucket, name=name, config=yaml_config)
    return {"success": True}
```

**Benefits**:
- ‚úÖ Validation logic unit testable
- ‚úÖ YAML generation unit testable
- ‚úÖ Admin API integration testable

**Estimated Effort**: 2-3 hours

---

### Summary: Refactoring Prerequisites

| Module | Function | Refactoring Needed | Effort | Priority |
|--------|----------|-------------------|--------|----------|
| quilt_service.py | `get_catalog_config()` | Split HTTP from logic | 2-3h | HIGH |
| quilt_service.py | `create_package_revision()` | Add dependency injection | 1-2h | HIGH |
| utils.py | `generate_signed_url()` | None - already good | 0h | N/A |
| tabulator_service.py | `create_table()` | Extract validation/YAML | 2-3h | MEDIUM |
| error_recovery.py | All functions | Extract testable logic | 3-4h | HIGH |
| workflow_service.py | Workflow functions | Extract state logic | 2-3h | MEDIUM |

**Total Estimated Effort**: 10-18 hours of refactoring before tests can be written

---

## Section 7: Test Infrastructure Requirements

### Required Libraries

#### Already Available (Confirmed via existing tests)
- ‚úÖ `pytest` - Test framework
- ‚úÖ `pytest-asyncio` - Async test support
- ‚úÖ `unittest.mock` - Basic mocking
- ‚úÖ `moto` - AWS service mocking (used in integration tests)
- ‚úÖ `boto3` - AWS SDK

#### Need to Add

##### 1. respx - HTTP Mocking for httpx
```bash
uv add --dev respx
```

**Use Case**: Mock HTTP endpoints for testing catalog config fetching

**Example**:
```python
import respx
import httpx

@respx.mock
def test_http_integration():
    respx.get("https://api.example.com/config").mock(
        return_value=httpx.Response(200, json={"key": "value"})
    )
```

**Why respx?**: Mocks HTTP responses without mocking the HTTP client itself

---

##### 2. freezegun - Time Travel for Tests
```bash
uv add --dev freezegun
```

**Use Case**: Test retry logic, timeout handling, expiration clamping

**Example**:
```python
from freezegun import freeze_time

@freeze_time("2024-01-01 12:00:00")
def test_retry_after_timeout():
    # Test that retries happen after expected time
```

---

##### 3. pytest-httpx (Alternative to respx)
```bash
uv add --dev pytest-httpx
```

**Use Case**: Alternative HTTP mocking, simpler fixture-based approach

**Decision**: Choose either respx OR pytest-httpx, not both

---

### Test Fixtures Needed

#### 1. Integration Test Fixtures for HTTP

```python
# tests/integration/conftest.py

import pytest
import respx
from quilt_mcp.services.quilt_service import QuiltService

@pytest.fixture
def mock_catalog_http():
    """Mock catalog HTTP endpoints for integration tests."""
    with respx.mock:
        # Mock successful config.json response
        respx.get("https://test-catalog.com/config.json").mock(
            return_value=httpx.Response(
                200,
                json={
                    "region": "us-east-1",
                    "apiGatewayEndpoint": "https://api.test.com",
                    "analyticsBucket": "test-analyticsbucket-abc",
                }
            )
        )
        yield

@pytest.fixture
def quilt_service():
    """Create QuiltService instance for testing."""
    return QuiltService()
```

---

#### 2. Integration Test Fixtures for S3 (moto)

```python
# tests/integration/conftest.py

import boto3
import pytest
from moto import mock_s3

@pytest.fixture
def mock_s3_bucket():
    """Create mock S3 bucket with moto."""
    with mock_s3():
        s3 = boto3.client("s3", region_name="us-east-1")
        s3.create_bucket(Bucket="test-bucket")

        # Add test objects
        s3.put_object(Bucket="test-bucket", Key="file1.csv", Body=b"data1")
        s3.put_object(Bucket="test-bucket", Key="file2.json", Body=b"data2")

        yield s3
```

---

#### 3. Integration Test Fixtures for Athena (moto)

```python
# tests/integration/conftest.py

from moto import mock_athena

@pytest.fixture
def mock_athena_catalog():
    """Create mock Athena catalog with moto."""
    with mock_athena():
        athena = boto3.client("athena", region_name="us-east-1")

        # Create test data catalog
        athena.create_data_catalog(
            Name="test-catalog",
            Type="GLUE",
        )

        yield athena
```

---

### Test Data Requirements

#### 1. Sample Catalog Config Responses

Create `tests/fixtures/catalog_configs/`:

```
tests/fixtures/catalog_configs/
‚îú‚îÄ‚îÄ nightly_quilttest_com.json       # Full config from nightly
‚îú‚îÄ‚îÄ production_quiltdata_com.json    # Full config from prod
‚îú‚îÄ‚îÄ minimal_config.json               # Minimal valid config
‚îú‚îÄ‚îÄ invalid_config.json               # Invalid config for error testing
‚îî‚îÄ‚îÄ README.md                         # Documentation
```

**Content**:
```json
// nightly_quilttest_com.json
{
  "region": "us-east-1",
  "apiGatewayEndpoint": "https://0xrvxq2hb8.execute-api.us-east-1.amazonaws.com/prod",
  "analyticsBucket": "quilt-staging-analyticsbucket-10ort3e91tnoa",
  "sentryDSN": "https://sentry.io/123",
  "mixpanelToken": "token123",
  "alwaysRequiresAuth": true,
  ...
}
```

---

#### 2. Sample S3 Objects and Metadata

Create `tests/fixtures/s3_objects/`:

```
tests/fixtures/s3_objects/
‚îú‚îÄ‚îÄ sample_package_manifest.json     # Real package manifest
‚îú‚îÄ‚îÄ sample_csv_data.csv              # Sample CSV for testing
‚îú‚îÄ‚îÄ sample_json_data.json            # Sample JSON
‚îî‚îÄ‚îÄ README.md
```

---

#### 3. Sample Athena Query Responses

Create `tests/fixtures/athena_responses/`:

```
tests/fixtures/athena_responses/
‚îú‚îÄ‚îÄ show_databases.json              # Sample database list
‚îú‚îÄ‚îÄ show_tables.json                 # Sample table list
‚îú‚îÄ‚îÄ select_query_result.json         # Sample query result
‚îî‚îÄ‚îÄ README.md
```

---

### Performance Considerations

#### Test Execution Speed Targets

| Test Type | Target Speed | Acceptable Range | Notes |
|-----------|-------------|------------------|-------|
| Unit tests | <1ms each | 0.1-2ms | Pure logic, no I/O |
| Integration (HTTP) | <50ms each | 10-100ms | respx mocking |
| Integration (S3) | <100ms each | 50-200ms | moto mocking |
| Integration (Athena) | <200ms each | 100-300ms | moto mocking |

#### Total Test Suite Speed

**Current**:
- Unit tests: ~2s (527 tests √ó ~4ms avg)
- Integration tests: ~30s
- E2E tests: ~60s
- **Total**: ~92s

**After Refactoring**:
- Unit tests: ~1.5s (reduce from 527 to ~400 tests)
- Integration tests: ~60s (add 150+ new integration tests)
- E2E tests: ~60s (unchanged)
- **Total**: ~121.5s (+32%)

**Recommendation**: Acceptable increase given coverage gains

---

#### Optimization Strategies

1. **Parallel Execution**
   ```bash
   pytest -n auto  # Use pytest-xdist for parallel execution
   ```

2. **Fixture Caching**
   ```python
   @pytest.fixture(scope="session")  # Reuse across tests
   def expensive_fixture():
       ...
   ```

3. **Fast Fail**
   ```bash
   pytest -x  # Stop on first failure during development
   ```

4. **Selective Testing**
   ```bash
   pytest tests/unit  # Run only unit tests
   pytest -m "not slow"  # Skip slow tests
   ```

---

### Infrastructure Setup Checklist

- [ ] Add respx to dev dependencies
- [ ] Add freezegun to dev dependencies
- [ ] Create `tests/fixtures/catalog_configs/` directory
- [ ] Create `tests/fixtures/s3_objects/` directory
- [ ] Create `tests/fixtures/athena_responses/` directory
- [ ] Add HTTP fixtures to `tests/integration/conftest.py`
- [ ] Add S3 fixtures (moto) to conftest
- [ ] Add Athena fixtures (moto) to conftest
- [ ] Document fixture usage in `tests/README.md`
- [ ] Configure pytest for parallel execution
- [ ] Set up pytest markers for slow tests

---

## What Needs to Be Done (Implementation Roadmap)

### Phase 2A: Refactoring Prerequisites (Week 1)

**Goal**: Make code testable without changing behavior

1. **Refactor quilt_service.py** (Priority: HIGH)
   - [ ] Split `get_catalog_config()` into 3 functions
     - [ ] `_fetch_catalog_config()` - HTTP layer
     - [ ] `_filter_catalog_config()` - Filtering logic
     - [ ] `_derive_stack_prefix()` - Derivation logic
   - [ ] Add dependency injection to `create_package_revision()`
   - [ ] Extract `_organize_s3_files_smart()` testable logic
   - [ ] Extract `_collect_objects_flat()` testable logic

2. **Refactor tabulator_service.py** (Priority: MEDIUM)
   - [ ] Extract `_validate_table_config()` from `create_table()`
   - [ ] Extract `_normalize_parser_config()` pure logic
   - [ ] Extract `_generate_table_yaml()` YAML generation

3. **Refactor error_recovery.py** (Priority: HIGH - biggest gain)
   - [ ] Extract pure retry logic functions
   - [ ] Separate timeout handling from I/O
   - [ ] Make error handlers testable

**Estimated Effort**: 10-18 hours

**Success Criteria**: All existing tests still pass, no behavior changes

---

### Phase 2B: Test Infrastructure (Week 1-2)

**Goal**: Set up fixtures and libraries

4. **Add Dependencies**
   - [ ] Add respx for HTTP mocking
   - [ ] Add freezegun for time testing
   - [ ] Update pyproject.toml

5. **Create Test Fixtures**
   - [ ] Create `tests/fixtures/catalog_configs/` with 4 JSON files
   - [ ] Create `tests/fixtures/s3_objects/` with sample data
   - [ ] Create `tests/fixtures/athena_responses/` with sample responses

6. **Add Integration Fixtures**
   - [ ] Add `mock_catalog_http` fixture to conftest.py
   - [ ] Add `mock_s3_bucket` fixture (enhance existing)
   - [ ] Add `mock_athena_catalog` fixture

**Estimated Effort**: 4-6 hours

---

### Phase 2C: Write Integration Tests (Week 2-3)

**Goal**: Replace mocked tests with integration tests

7. **quilt_service.py Integration Tests** (HIGH PRIORITY)
   - [ ] Create `tests/integration/test_quilt_service_http.py`
     - [ ] `test_get_catalog_config_success()`
     - [ ] `test_get_catalog_config_timeout()`
     - [ ] `test_get_catalog_config_404()`
     - [ ] `test_get_catalog_config_invalid_json()`
   - [ ] Create `tests/integration/test_quilt_service_packages.py`
     - [ ] `test_create_package_revision_auto_organize_true()`
     - [ ] `test_create_package_revision_auto_organize_false()`
     - [ ] `test_create_package_revision_with_metadata()`
     - [ ] `test_create_package_revision_error_handling()`

8. **error_recovery.py Integration Tests** (HIGH PRIORITY)
   - [ ] Create `tests/integration/test_error_recovery.py`
     - [ ] Test retry workflows with real failures
     - [ ] Test timeout handling
     - [ ] Test error propagation

9. **tabulator_service.py Integration Tests** (MEDIUM PRIORITY)
   - [ ] Create `tests/integration/test_tabulator_integration.py`
     - [ ] `test_tabulator_query_with_real_athena()`
     - [ ] `test_create_table_with_admin_api()`

**Estimated Effort**: 15-20 hours

---

### Phase 2D: Convert Unit Tests (Week 3-4)

**Goal**: Convert over-mocked tests to pure logic tests

10. **Convert test_quilt_service.py Tests**
    - [ ] Keep: Pure logic tests (admin checks, return value tests)
    - [ ] Convert: `test_get_catalog_config_*` ‚Üí unit tests for filtering logic
    - [ ] Delete: Mock interaction tests (browse, create_bucket)

11. **Keep test_utils.py Tests**
    - [ ] Keep: All `test_parse_s3_uri_*` tests (already good)
    - [ ] Reduce mocking: `test_generate_signed_url_*` tests

12. **Convert test_tabulator.py Tests**
    - [ ] Convert: Query tests ‚Üí integration tests
    - [ ] Keep: Validation tests as unit tests

**Estimated Effort**: 8-12 hours

---

### Phase 2E: Verification (Week 4)

**Goal**: Verify coverage gains and test quality

13. **Run Coverage Analysis**
    - [ ] Run: `make test-all-coverage`
    - [ ] Verify: Coverage increased by 2.9-3.8%
    - [ ] Verify: Integration coverage increased for target modules

14. **Quality Checks**
    - [ ] All tests pass
    - [ ] No behavior changes in production code
    - [ ] Test suite runs in <150s
    - [ ] No flaky tests

**Estimated Effort**: 4-6 hours

---

### Summary: Implementation Roadmap

| Phase | Tasks | Effort | Priority |
|-------|-------|--------|----------|
| 2A: Refactoring | 3 major refactorings | 10-18h | HIGH |
| 2B: Infrastructure | Fixtures, dependencies | 4-6h | HIGH |
| 2C: Integration Tests | Write 25+ new tests | 15-20h | HIGH |
| 2D: Convert Unit Tests | Reduce mocking | 8-12h | MEDIUM |
| 2E: Verification | Coverage analysis | 4-6h | HIGH |

**Total Estimated Effort**: 41-62 hours (1-1.5 weeks full-time)

**Expected Outcome**: +2.9-3.8% overall coverage, +23-33% on target modules

---

## Success Criteria

This Phase 2 implementation will be successful when:

### Quantitative Criteria

- [x] All 4 test files analyzed (test_quilt_service.py, test_utils.py, test_tabulator.py, test_selector_fn.py)
- [ ] **Overall coverage increases from 55.7% to 58.6-59.5%** (+2.9-3.8%)
- [ ] **quilt_service.py coverage increases from 83.3% to 91.9%** (+8.6%)
- [ ] **error_recovery.py coverage increases from 59.9% to 95.3%** (+35.4%)
- [ ] **Integration coverage for target modules increases by 25-40%**
- [ ] Test suite runs in <150 seconds
- [ ] Zero flaky tests

### Qualitative Criteria

- [ ] Unit tests focus on **pure logic** (no I/O, no mocking)
- [ ] Integration tests focus on **component interaction** (real HTTP, real S3, real Athena)
- [ ] Tests verify **behavior**, not mock interactions
- [ ] Code is refactored for **testability** (logic separated from I/O)
- [ ] Test infrastructure supports **future growth**

### Documentation Criteria

- [ ] All test fixtures documented in `tests/README.md`
- [ ] Testing philosophy documented
- [ ] Examples provided for future test writing

---

## Recommendations

### Immediate Actions (This Week)

1. **Review and approve this analysis** ‚úÖ (You're reading it!)
2. **Start Phase 2A refactoring** (quilt_service.py splitting)
3. **Add respx and freezegun dependencies**
4. **Create test fixture directories**

### Priority Order

1. **HIGH**: quilt_service.py refactoring and integration tests (+8.6%)
2. **HIGH**: error_recovery.py integration tests (+35.4% - biggest win!)
3. **MEDIUM**: workflow_service.py integration tests (+31.9%)
4. **MEDIUM**: governance_service.py integration tests (+25.8%)
5. **LOW**: utils.py improvements (+5% - already has integration tests)

### What NOT to Do

- ‚ùå Do NOT try to achieve 100% coverage
- ‚ùå Do NOT write integration tests for pure logic functions
- ‚ùå Do NOT mock internal application code (only external services)
- ‚ùå Do NOT skip refactoring step (will make tests brittle)
- ‚ùå Do NOT delete tests without replacement integration tests

---

## Conclusion

This analysis reveals a **systematic over-mocking problem** across 1,577 lines of unit tests. The core issue: tests verify that mocks were called, not that the code works.

### Key Findings

1. **109 mocks in test_quilt_service.py** bypass HTTP, session management, and package operations
2. **Coverage is misleading**: 82.6% unit coverage masks 21.5% integration coverage (61.1% gap)
3. **Potential impact**: +2.9-3.8% overall coverage, +23-33% on critical modules
4. **Biggest opportunity**: error_recovery.py has 0% integration coverage, potential +35.4% gain

### What's Worth Doing

**‚úÖ Worth Doing**:
- Refactor quilt_service.py for testability (HIGH ROI)
- Write integration tests for error_recovery.py (+35.4% gain!)
- Convert HTTP mocking to respx-based integration tests
- Keep utils.py as success model (already has integration tests)

**‚ö†Ô∏è Not Worth Doing**:
- Trying to achieve 100% coverage
- Converting pure logic tests (test_parse_s3_uri_* are perfect as-is)
- Writing integration tests for simple validators

### Next Steps

1. **Approve this analysis** and roadmap
2. **Start Phase 2A**: Refactor quilt_service.py (10-18 hours)
3. **Implement Phase 2B**: Set up test infrastructure (4-6 hours)
4. **Execute Phase 2C**: Write integration tests (15-20 hours)
5. **Verify Phase 2E**: Confirm coverage gains (4-6 hours)

**Total effort**: 41-62 hours over 1-1.5 weeks

**Expected outcome**: Meaningful coverage increase focusing on critical service modules, with tests that actually verify behavior rather than mock interactions.

---

## Appendix: Reference Data

### Coverage Data Source
- File: `build/test-results/coverage-analysis.csv`
- Date: 2025-11-15
- Overall coverage: 55.7%
- Target coverage: 75%+

### Test File Statistics
- test_quilt_service.py: 741 lines, 109 mocks
- test_utils.py: 526 lines, 48 mocks
- test_tabulator.py: 204 lines, 31 mocks
- test_selector_fn.py: 106 lines, 23 mocks
- **Total**: 1,577 lines, 211 mock references

### Integration Tests Inventory
- tests/integration/test_utils_integration.py ‚úÖ
- tests/integration/test_bucket_tools.py ‚úÖ
- tests/integration/test_s3_package.py ‚úÖ
- tests/integration/test_athena.py ‚úÖ
- tests/integration/test_permissions.py ‚úÖ
- **Missing**: HTTP tests, package abstraction tests, tabulator tests

---

**Document Version**: 1.0
**Author**: Analysis Agent
**Date**: 2025-11-15
**Status**: Complete - Ready for Review
