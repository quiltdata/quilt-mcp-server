# Smart Search Validation in mcp-test.py

**Date:** 2025-01-13
**Status:** Specification
**Context:** Improving mcp-test.py to intelligently validate search results

---

## Problem Statement

Currently, `mcp-test.py` performs **shallow validation** of search tools:

```python
# Current behavior - tests that tools don't error, but NOT that they return correct results
result = call_tool("search_catalog", {"query": "README.md", "scope": "bucket"})
# ‚úÖ Passes if no error
# ‚ùå Doesn't verify README.md is actually in results
```

**This is insufficient.** We need **smart validation** that ensures search tools return **expected data**.

---

## Requirements

### R1: Search Variants Must Return Expected Results

**Given** test configuration specifies:
- `TEST_ENTRY = "README.md"` (a file in the test bucket)
- `TEST_PACKAGE = "examples/wellplates"` (a package in the test catalog)

**Then** search tests must verify:

| Search Variant | Expected Results |
|----------------|------------------|
| `scope="bucket"` | Must contain `TEST_ENTRY` ("README.md") |
| `scope="package"` | Must contain `TEST_PACKAGE` ("examples/wellplates") |
| `scope="global"` | Must contain BOTH `TEST_ENTRY` AND `TEST_PACKAGE` |

### R2: Validation Must Be Smart, Not Brittle

**Don't just check existence** - check that results **actually match the query**:

```python
# ‚ùå BRITTLE: Fails if result ordering changes
assert results[0]["key"] == "README.md"

# ‚úÖ SMART: Checks that README.md appears anywhere in results
assert any("README.md" in r.get("key", "") for r in results)
```

### R3: Configuration Drives Validation

Test validation logic should be **generated by mcp-list.py** and stored in `mcp-test.yaml`:

```yaml
test_tools:
  search_catalog.bucket:
    tool: search_catalog
    arguments:
      query: "README.md"
      scope: "bucket"
      target: "s3://quilt-example"
      limit: 10

    # NEW: Expected validation
    validation:
      type: "search"
      must_contain_entry: "README.md"  # From TEST_ENTRY
      result_field: "key"               # Which field to check
      min_results: 1                    # At least 1 result

  search_catalog.package:
    tool: search_catalog
    arguments:
      query: "wellplates"
      scope: "package"
      limit: 10

    validation:
      type: "search"
      must_contain_package: "examples/wellplates"  # From TEST_PACKAGE
      result_field: "name"                         # Package name field
      min_results: 1

  search_catalog.global:
    tool: search_catalog
    arguments:
      query: "*"
      scope: "global"
      limit: 50

    validation:
      type: "search"
      must_contain_entry: "README.md"
      must_contain_package: "examples/wellplates"
      result_fields: ["key", "name"]  # Check both object keys and package names
      min_results: 2                  # At least 2 (one of each type)
```

---

## Design

### Component 1: Enhanced YAML Schema

**File:** `scripts/tests/mcp-test.yaml` (generated by `mcp-list.py`)

```yaml
# Environment config (existing)
environment:
  QUILT_DEFAULT_BUCKET: "s3://quilt-example"
  QUILT_TEST_PACKAGE: "examples/wellplates"
  QUILT_TEST_ENTRY: "README.md"

# Tool tests (enhanced with validation)
test_tools:
  search_catalog.bucket:
    tool: search_catalog
    description: "Search within specific bucket for objects"
    effect: none
    arguments:
      query: "README.md"      # Search for the test entry
      scope: "bucket"
      target: "s3://quilt-example"
      limit: 10

    # Response schema (existing)
    response_schema:
      type: object
      required: ["content"]
      properties:
        content:
          type: array

    # NEW: Smart validation rules
    validation:
      type: "search"
      description: "Bucket search must return TEST_ENTRY"

      # What we expect to find
      must_contain:
        - value: "README.md"          # From TEST_ENTRY env var
          field: "key"                 # Check in result["key"]
          match_type: "substring"      # "README.md" should appear in key

      # Minimum results (at least 1)
      min_results: 1

      # Optional: Result shape validation
      result_shape:
        required_fields: ["key", "size", "updated"]

  search_catalog.package:
    tool: search_catalog
    description: "Search packages in catalog"
    effect: none
    arguments:
      query: "wellplates"
      scope: "package"
      limit: 10

    validation:
      type: "search"
      description: "Package search must return TEST_PACKAGE"

      must_contain:
        - value: "examples/wellplates"  # From TEST_PACKAGE env var
          field: "name"                  # Check in result["name"]
          match_type: "exact"            # Exact match for package names

      min_results: 1
      result_shape:
        required_fields: ["name", "topHash", "modified"]

  search_catalog.global:
    tool: search_catalog
    description: "Global search across all entities"
    effect: none
    arguments:
      query: "*"              # Broad search to get everything
      scope: "global"
      limit: 50

    validation:
      type: "search"
      description: "Global search must return both TEST_ENTRY and TEST_PACKAGE"

      # Must find BOTH the file AND the package
      must_contain:
        - value: "README.md"
          field: "key"
          match_type: "substring"
          description: "Must find TEST_ENTRY in results"

        - value: "examples/wellplates"
          field: "name"
          match_type: "exact"
          description: "Must find TEST_PACKAGE in results"

      min_results: 2  # At least one of each type

      # Optional: Check result diversity
      result_diversity:
        must_have_types: ["object", "package"]  # If results have "type" field
```

### Component 2: Validation Engine in mcp-test.py

**File:** `scripts/mcp-test.py`

```python
class SearchValidator:
    """Validates search results against expected outcomes."""

    def __init__(self, validation_config: Dict[str, Any], env_vars: Dict[str, str]):
        """Initialize validator with config and environment.

        Args:
            validation_config: Validation rules from YAML
            env_vars: Environment variables for substitution
        """
        self.config = validation_config
        self.env_vars = env_vars

    def validate(self, result: Dict[str, Any]) -> tuple[bool, Optional[str]]:
        """Validate search result.

        Returns:
            (is_valid, error_message)
            - is_valid: True if validation passed
            - error_message: None if valid, error string if invalid
        """
        validation_type = self.config.get("type")

        if validation_type == "search":
            return self._validate_search(result)
        else:
            # Unknown validation type - skip
            return True, None

    def _validate_search(self, result: Dict[str, Any]) -> tuple[bool, Optional[str]]:
        """Validate search-specific results."""

        # Extract results array from response
        # MCP tools return {"content": [...]} format
        content = result.get("content", [])
        if not content:
            return False, "Empty response content"

        # Parse the actual results (usually JSON string in content[0]["text"])
        try:
            if isinstance(content[0], dict) and "text" in content[0]:
                import json
                search_results = json.loads(content[0]["text"])
            else:
                search_results = content[0]

            results_list = search_results.get("results", [])
        except (json.JSONDecodeError, KeyError, IndexError) as e:
            return False, f"Failed to parse search results: {e}"

        # Check minimum results
        min_results = self.config.get("min_results", 0)
        if len(results_list) < min_results:
            return False, f"Expected at least {min_results} results, got {len(results_list)}"

        # Check must_contain rules
        must_contain = self.config.get("must_contain", [])
        for rule in must_contain:
            is_found, error = self._check_must_contain(results_list, rule)
            if not is_found:
                return False, error

        # Check result shape if specified
        result_shape = self.config.get("result_shape")
        if result_shape:
            shape_valid, shape_error = self._validate_result_shape(results_list, result_shape)
            if not shape_valid:
                return False, shape_error

        # All checks passed
        return True, None

    def _check_must_contain(
        self,
        results: List[Dict],
        rule: Dict[str, str]
    ) -> tuple[bool, Optional[str]]:
        """Check if results contain expected value.

        Args:
            results: List of result dictionaries
            rule: must_contain rule with value, field, match_type

        Returns:
            (is_found, error_message)
        """
        expected_value = rule["value"]
        field_name = rule["field"]
        match_type = rule.get("match_type", "substring")
        description = rule.get("description", f"Expected to find '{expected_value}'")

        # Search through results
        found = False
        for result in results:
            actual_value = result.get(field_name, "")

            if match_type == "exact":
                if actual_value == expected_value:
                    found = True
                    break
            elif match_type == "substring":
                if expected_value in str(actual_value):
                    found = True
                    break
            elif match_type == "regex":
                import re
                if re.search(expected_value, str(actual_value)):
                    found = True
                    break

        if not found:
            # Generate helpful error message
            error = f"{description}\n"
            error += f"  Expected: '{expected_value}' in field '{field_name}'\n"
            error += f"  Match type: {match_type}\n"
            error += f"  Searched {len(results)} results\n"

            # Show sample of what we found instead
            if results and len(results) > 0:
                sample = results[:3]
                sample_values = [r.get(field_name, "<missing>") for r in sample]
                error += f"  Sample values: {sample_values}"

            return False, error

        return True, None

    def _validate_result_shape(
        self,
        results: List[Dict],
        shape: Dict[str, Any]
    ) -> tuple[bool, Optional[str]]:
        """Validate that results have expected shape.

        Args:
            results: List of result dictionaries
            shape: Expected shape with required_fields, optional_fields, etc.

        Returns:
            (is_valid, error_message)
        """
        if not results:
            return True, None  # Empty results are OK if we got this far

        required_fields = shape.get("required_fields", [])

        # Check first result (representative sample)
        first_result = results[0]
        missing_fields = [f for f in required_fields if f not in first_result]

        if missing_fields:
            return False, f"Results missing required fields: {missing_fields}"

        return True, None
```

### Component 3: Integration with ToolsTester

**File:** `scripts/mcp-test.py` (modify existing ToolsTester)

```python
class ToolsTester(MCPTester):
    """Tool testing subclass with smart validation."""

    def __init__(self, config: Dict[str, Any] = None, **kwargs):
        super().__init__(**kwargs)
        self.config = config or {}
        self.results = TestResults()

        # NEW: Store environment variables for validation
        self.env_vars = config.get("environment", {})

    def run_test(self, tool_name: str, test_config: Dict[str, Any]) -> None:
        """Run a single tool test with smart validation."""
        try:
            print(f"\n--- Testing tool: {tool_name} ---")

            # Get test arguments
            test_args = test_config.get("arguments", {})
            actual_tool_name = test_config.get("tool", tool_name)

            # Call the tool
            result = self.call_tool(actual_tool_name, test_args)

            # Schema validation (existing)
            if "response_schema" in test_config:
                validate(result, test_config["response_schema"])
                self._log("‚úÖ Response schema validation passed")

            # NEW: Smart validation for search tools
            if "validation" in test_config:
                validator = SearchValidator(
                    test_config["validation"],
                    self.env_vars
                )
                is_valid, error_msg = validator.validate(result)

                if not is_valid:
                    # Validation failed - record detailed error
                    print(f"‚ùå {tool_name}: VALIDATION FAILED")
                    print(f"   {error_msg}")

                    self.results.record_failure({
                        "name": tool_name,
                        "actual_tool": actual_tool_name,
                        "arguments": test_args,
                        "error": f"Smart validation failed: {error_msg}",
                        "error_type": "ValidationError",
                        "result_summary": self._summarize_result(result)
                    })
                    return
                else:
                    self._log(f"‚úÖ Smart validation passed: {test_config['validation'].get('description', 'OK')}")

            # Success!
            print(f"‚úÖ {tool_name}: PASSED")
            self.results.record_pass({
                "name": tool_name,
                "actual_tool": actual_tool_name,
                "arguments": test_args,
                "result": result,
                "validation": "passed" if "validation" in test_config else "schema_only"
            })

        except Exception as e:
            print(f"‚ùå {tool_name}: FAILED - {e}")
            self.results.record_failure({
                "name": tool_name,
                "actual_tool": test_config.get("tool", tool_name),
                "arguments": test_config.get("arguments", {}),
                "error": str(e),
                "error_type": type(e).__name__
            })

    def _summarize_result(self, result: Dict[str, Any]) -> str:
        """Generate human-readable summary of result for error reporting."""
        try:
            content = result.get("content", [])
            if content and isinstance(content[0], dict) and "text" in content[0]:
                import json
                data = json.loads(content[0]["text"])
                results = data.get("results", [])
                return f"{len(results)} results returned"
            return "Unknown result format"
        except:
            return "Could not parse result"
```

### Component 4: Generation Logic in mcp-list.py

**File:** `scripts/mcp-list.py` (modify `generate_test_yaml()`)

```python
async def generate_test_yaml(server, output_file: str, env_vars: Dict[str, str | None]):
    """Generate mcp-test.yaml with smart validation."""

    # ... existing code ...

    # Load test values from environment
    test_package: str = env_vars.get("QUILT_TEST_PACKAGE") or "examples/wellplates"
    test_entry: str = env_vars.get("QUILT_TEST_ENTRY") or "README.md"
    default_bucket: str = env_vars.get("QUILT_DEFAULT_BUCKET") or "s3://quilt-example"

    # Define search tool variants with smart validation
    search_variants = {
        "search_catalog.bucket": {
            "tool": "search_catalog",
            "arguments": {
                "query": test_entry,  # Search for test file
                "scope": "bucket",
                "target": default_bucket,
                "limit": 10
            },
            "validation": {
                "type": "search",
                "description": f"Bucket search must return TEST_ENTRY ({test_entry})",
                "must_contain": [
                    {
                        "value": test_entry,
                        "field": "key",
                        "match_type": "substring",
                        "description": f"Must find {test_entry} in bucket search results"
                    }
                ],
                "min_results": 1,
                "result_shape": {
                    "required_fields": ["key", "size"]
                }
            }
        },

        "search_catalog.package": {
            "tool": "search_catalog",
            "arguments": {
                "query": test_package.split("/")[-1],  # Search for package name
                "scope": "package",
                "limit": 10
            },
            "validation": {
                "type": "search",
                "description": f"Package search must return TEST_PACKAGE ({test_package})",
                "must_contain": [
                    {
                        "value": test_package,
                        "field": "name",
                        "match_type": "substring",
                        "description": f"Must find {test_package} in package search results"
                    }
                ],
                "min_results": 1,
                "result_shape": {
                    "required_fields": ["name", "topHash"]
                }
            }
        },

        "search_catalog.global": {
            "tool": "search_catalog",
            "arguments": {
                "query": "*",  # Broad search
                "scope": "global",
                "limit": 50
            },
            "validation": {
                "type": "search",
                "description": "Global search must return both TEST_ENTRY and TEST_PACKAGE",
                "must_contain": [
                    {
                        "value": test_entry,
                        "field": "key",
                        "match_type": "substring",
                        "description": f"Must find TEST_ENTRY ({test_entry}) in global results"
                    },
                    {
                        "value": test_package,
                        "field": "name",
                        "match_type": "substring",
                        "description": f"Must find TEST_PACKAGE ({test_package}) in global results"
                    }
                ],
                "min_results": 2  # At least one of each
            }
        }
    }

    # Add to test config
    for variant_name, variant_config in search_variants.items():
        test_config["test_tools"][variant_name] = variant_config

    # ... rest of generation ...
```

---

## Implementation Plan

### Phase 1: Validation Schema (1 hour)

1. ‚úÖ Design YAML validation schema
2. ‚úÖ Document validation rules
3. ‚úÖ Create examples

### Phase 2: SearchValidator Class (2 hours)

1. Implement `SearchValidator` class
2. Add `_validate_search()` method
3. Add `_check_must_contain()` method
4. Add `_validate_result_shape()` method
5. Write unit tests for validator

### Phase 3: Integration with ToolsTester (1 hour)

1. Modify `ToolsTester.run_test()` to use validator
2. Add error reporting for validation failures
3. Update test result tracking

### Phase 4: Generation in mcp-list.py (1 hour)

1. Modify `generate_test_yaml()` to generate validation rules
2. Add search-specific variant generation
3. Test YAML generation with real data

### Phase 5: Testing and Refinement (2 hours)

1. Run generated tests against live MCP server
2. Refine validation rules based on actual results
3. Add more robust error messages
4. Handle edge cases

**Total Estimated Time:** 7 hours

---

## Success Criteria

### C1: Tests Fail When Expected Data Missing

```bash
# Remove README.md from test bucket
$ aws s3 rm s3://quilt-example/examples/wellplates/README.md

# Run tests
$ python scripts/mcp-test.py --tools-test

# Expected output:
‚ùå search_catalog.bucket: VALIDATION FAILED
   Must find README.md in bucket search results
   Expected: 'README.md' in field 'key'
   Match type: substring
   Searched 10 results
   Sample values: ['.timestamp', 'data.csv', 'metadata.json']
```

### C2: Tests Pass When Expected Data Present

```bash
# With README.md present
$ python scripts/mcp-test.py --tools-test

# Expected output:
‚úÖ search_catalog.bucket: PASSED
‚úÖ Smart validation passed: Bucket search must return TEST_ENTRY (README.md)
```

### C3: Global Search Validates Both Types

```bash
# Expected output for global search:
‚úÖ search_catalog.global: PASSED
‚úÖ Smart validation passed: Global search must return both TEST_ENTRY and TEST_PACKAGE
   - Found README.md in results
   - Found examples/wellplates in results
```

### C4: Helpful Error Messages

When validation fails, output must be **actionable**:

```
‚ùå search_catalog.package: VALIDATION FAILED
   Must find examples/wellplates in package search results
   Expected: 'examples/wellplates' in field 'name'
   Match type: substring
   Searched 5 results
   Sample values: ['examples/other', 'test/package', 'user/data']

   üí° Troubleshooting:
      - Verify QUILT_TEST_PACKAGE is set correctly in .env
      - Check that package exists: quilt3 list examples/wellplates
      - Ensure search index is up to date
```

---

## Edge Cases and Error Handling

### E1: Empty Results

```python
# If search returns no results
{
  "results": [],
  "total_results": 0
}

# Validation should report:
‚ùå Expected at least 1 results, got 0
   This likely means:
   - Search query is too restrictive
   - Test data (TEST_ENTRY/TEST_PACKAGE) doesn't exist
   - Search index needs reindexing
```

### E2: Malformed Results

```python
# If search returns unexpected format
{
  "results": [
    {"key": "file1.txt"},  # Missing "size" field
    {"key": "file2.txt"}
  ]
}

# Validation should report:
‚ùå Results missing required fields: ['size']
   Result shape validation failed
   First result: {"key": "file1.txt"}
   Expected fields: ["key", "size"]
```

### E3: Partial Matches

```python
# If only one of two expected items found
# (e.g., found TEST_ENTRY but not TEST_PACKAGE)

‚ùå search_catalog.global: VALIDATION FAILED
   Must find examples/wellplates in package search results
   Expected: 'examples/wellplates' in field 'name'

   ‚úÖ Found 1/2 required items:
      ‚úÖ README.md (in field 'key')
      ‚ùå examples/wellplates (in field 'name') <- MISSING

   This suggests:
   - Package search may be failing
   - TEST_PACKAGE may not exist in catalog
```

---

## Benefits

### B1: Catch Real Issues

**Before (shallow validation):**
- Tool runs without error ‚úÖ
- Returns empty results
- Test passes ‚ùå (false positive)

**After (smart validation):**
- Tool runs without error ‚úÖ
- Returns empty results
- Validation fails: "Expected README.md in results" ‚úÖ
- Test fails ‚úÖ (correct behavior)

### B2: Self-Documenting Tests

Validation rules make tests **self-documenting**:

```yaml
validation:
  description: "Bucket search must return TEST_ENTRY (README.md)"
  must_contain:
    - value: "README.md"
      field: "key"
      description: "Must find README.md in bucket search results"
```

Anyone reading the YAML understands:
- What the test expects
- Why it expects it
- How to diagnose failures

### B3: Environment-Agnostic

Tests adapt to different environments via env vars:

```bash
# Development
QUILT_TEST_PACKAGE=examples/wellplates
QUILT_TEST_ENTRY=README.md

# CI
QUILT_TEST_PACKAGE=test/ci-package
QUILT_TEST_ENTRY=.ci-marker

# Production
QUILT_TEST_PACKAGE=prod/smoke-test
QUILT_TEST_ENTRY=health-check.txt
```

Same test code, different validation targets.

---

## Future Extensions

### F1: Content Validation

```yaml
validation:
  type: "search"
  must_contain:
    - value: "README.md"
      field: "key"

  # NEW: Validate file content
  content_validation:
    - result_filter: {"key": "README.md"}
      fetch_content: true
      content_must_contain: "# Examples"
      content_type: "text/markdown"
```

### F2: Performance Thresholds

```yaml
validation:
  type: "search"
  performance:
    max_query_time_ms: 1000  # Search must complete within 1s
    max_result_size_mb: 10    # Results must be under 10MB
```

### F3: Statistical Validation

```yaml
validation:
  type: "search"
  statistics:
    min_result_diversity: 0.7   # At least 70% unique keys
    min_relevance_score: 0.5    # Average relevance >= 0.5
```

---

## Summary

This spec defines a **smart validation system** for `mcp-test.py` that:

1. **Validates correctness**, not just absence of errors
2. **Adapts to environments** via configuration
3. **Provides actionable feedback** when tests fail
4. **Integrates seamlessly** with existing test infrastructure
5. **Scales easily** to new validation types

**Key files modified:**
- `scripts/mcp-test.py` - Add `SearchValidator` class
- `scripts/mcp-list.py` - Generate validation rules
- `scripts/tests/mcp-test.yaml` - Enhanced with validation config

**Impact:**
- **Catch real bugs** in search functionality
- **Prevent false positives** from shallow validation
- **Improve test reliability** across environments
- **Self-document** test expectations

---

**End of Specification**
