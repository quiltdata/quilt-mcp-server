# Intelligent Test Summary Design

## Problem: Current Summary is Confusing and Incomplete

### Current Issues

1. **Contradictory Information**

   ```
   ğŸ”§ Tools Tests:
      Total: 17
      âœ… Passed: 17
      âŒ Failed: 0    â† Says 0 failures

   BUT earlier output said:
      ğŸ“‹ Selected 17/48 tools for testing
      Skipped 31 non-idempotent tools
   ```

   **Problem**: "Failed: 0" is technically correct but misleading - we didn't test 31 tools at all!

2. **Missing Context in Final Summary**

   ```
   ================================================================================
      Tools: âœ… PASSED
      Resources: âŒ FAILED
      Overall: âŒ SOME TESTS FAILED
   ================================================================================
   ```

   **Problem**: No summary of what was tested vs skipped. Reader has to scroll up to find:
   - How many tools were selected vs total
   - Which tool categories were skipped
   - Resource template vs static breakdown

3. **Separation of Selection Info from Final Summary**

   ```
   [Beginning of output]
   ğŸ“‹ Selected 17/48 tools for testing
      Skipped 31 non-idempotent tools (configure: 6, create: 15, remove: 5, update: 5)
      Resources: 24 configured for testing

   [150 lines of test output]

   [End of output]
   ================================================================================
   ğŸ“Š OVERALL TEST SUMMARY
   ================================================================================
   [no mention of selection stats]
   ```

   **Problem**: Key context is separated from results by hundreds of lines

4. **Unclear What "Failed" Means for Resources**

   ```
   Failed Resources (7):
   â€¢ permissions://buckets/{bucket}/access
     Error: Template not found in server resourceTemplates
   ```

   **Problem**: Is this a test failure or expected for a resource template that needs registration?

---

## Solution: Contextual, Hierarchical Summary

### Design Principles

1. **Show What Was Run**: Always distinguish "not tested" from "tested and failed"
2. **Provide Context**: Final summary repeats key selection statistics
3. **Explain Outcomes**: Differentiate failure types (server error, config error, expected missing)
4. **Hierarchical Detail**: Three levels of detail based on verbosity
5. **Visual Clarity**: Only show failure counts when >0; avoid red X for zero failures
6. **Consistent Formatting**: Same format for tools and resources (don't repeat "0 skipped" for resources)

---

## Proposed Output Format

### Level 1: Always Show (Default)

```
================================================================================
ğŸ“Š TEST SUITE SUMMARY
================================================================================

ğŸ”§ TOOLS (17/48 tested, 31 skipped)
   Selection: Idempotent only (configure: 6, create: 15, remove: 5, update: 5 skipped)
   Results: âœ… 17 passed

ğŸ—‚ï¸  RESOURCES (24/24 tested)
   Type Breakdown: 17 static URIs, 7 templates
   Results: âœ… 17 passed, âŒ 7 failed

   âš ï¸  Failed Resources (7 template validation errors):
      â€¢ 7 templates not registered with server (expected - registration required)
        - permissions://buckets/{bucket}/access
        - admin://users/{name}
        - athena://databases/{database}/tables
        - athena://databases/{database}/tables/{table}/schema
        - metadata://templates/{name}
        - workflow://workflows/{id}
        - tabulator://buckets/{bucket}/tables

   ğŸ“‹ Analysis:
      These templates exist in test config but weren't registered by server.
      This may be expected if:
      - Templates are dynamically registered based on runtime config
      - Features require additional setup (auth, buckets, etc.)

      To investigate:
      1. Check server logs for registration warnings
      2. Run with --verbose to see full server capabilities
      3. Verify feature flags and environment variables

================================================================================
   Overall Status: âš ï¸  PARTIAL PASS
   - All tested tools passed
   - Some resource templates not available on server
   - Core functionality verified
================================================================================

ğŸ’¡ To test skipped tools: python scripts/tests/test_mcp.py --all
ğŸ’¡ To see full details: python scripts/tests/test_mcp.py --verbose
```

### Level 2: Verbose Mode (--verbose)

Shows actual test inputs and outputs for all failures.

```
================================================================================
ğŸ“Š TEST SUITE SUMMARY (VERBOSE)
================================================================================

ğŸ”§ TOOLS (17/48 tested, 31 skipped)
   Selection: Idempotent only (configure: 6, create: 15, remove: 5, update: 5 skipped)
   Results: âœ… 17 passed

ğŸ—‚ï¸  RESOURCES (24/24 tested)
   Type Breakdown: 17 static URIs, 7 templates
   Results: âœ… 17 passed, âŒ 7 failed

   âŒ Failed Resources (7):

   1. permissions://buckets/{bucket}/access
      Input Variables:
         {
           "bucket": "quilt-ernest-staging"
         }
      Resolved URI: permissions://buckets/quilt-ernest-staging/access

      Expected Output:
         - Resource found in server's resourceTemplates
         - Response contains permission data

      Actual Output:
         Error: Template not found in server resourceTemplates

         Server advertised templates (7):
         - config://{key}
         - docs://{section}
         - system://{component}
         [... 4 more ...]

         Note: permissions://* templates NOT in list

      Root Cause: Server didn't register permissions templates
      Severity: âš ï¸ Warning (likely feature-gated)

   2. admin://users/{name}
      Input Variables:
         {
           "name": "test_user"
         }
      Resolved URI: admin://users/test_user

      Expected Output:
         - Resource found in server's resourceTemplates
         - Response contains user data

      Actual Output:
         Error: Template not found in server resourceTemplates

         Server advertised templates (7):
         - config://{key}
         - docs://{section}
         - system://{component}
         [... 4 more ...]

         Note: admin://* templates NOT in list

      Root Cause: Server didn't register admin templates
      Severity: âš ï¸ Warning (likely feature-gated)

   [... remaining 5 failures with same format ...]

   ğŸ“‹ Pattern Analysis:
      All 7 failures: Same root cause (template registration)
      Impact: âœ… Core functionality verified, âš ï¸ Optional features unavailable

================================================================================
   Overall Status: âš ï¸  PARTIAL PASS
   - Core functionality verified (17/17 tools, 17/17 static resources)
   - 7 optional templates not registered (expected - feature-gated)
================================================================================
```

### Level 3: Debug Mode (--debug or on failure)

Adds per-test details:

```
ğŸ—‚ï¸  RESOURCES (24 tested, 0 skipped)
   Results: âœ… 17 passed, âŒ 7 failed, â­ï¸ 0 skipped

   âœ… Passed Resources (17):
      Static URIs (17/17):
      â€¢ config://catalog âœ… (text/plain, 245 bytes)
      â€¢ docs://quick-start âœ… (text/markdown, 3.2KB)
      â€¢ system://info âœ… (application/json, valid schema)
      [... remaining 14 ...]

   âŒ Failed Resources (7):
      Template Validation Errors (7/7):

      â€¢ permissions://buckets/{bucket}/access
        Test Input: {bucket: "quilt-ernest-staging"}
        Expected: Template registered, URI resolves to permission data
        Actual: Template not in server's resourceTemplates array
        Root Cause: Server didn't register this template (likely feature-gated)

      â€¢ admin://users/{name}
        Test Input: {name: "test_user"}
        Expected: Template registered, URI resolves to user data
        Actual: Template not in server's resourceTemplates array
        Root Cause: Server didn't register this template (likely feature-gated)

      [... remaining 5 with same pattern ...]

   ğŸ“Š Failure Pattern Analysis:
      All 7 failures are the same issue: Template registration

      Root Cause Hypothesis:
      1. These templates may require server-side configuration
      2. Features might be gated behind environment variables
      3. Dynamic registration may occur after first use

      Recommended Actions:
      1. âœ… Static resources all work - core MCP protocol OK
      2. ğŸ” Check server logs for template registration messages
      3. ğŸ”§ Review feature flags in config (SSO_ENABLED, ADMIN_API_ENABLED, etc.)
      4. ğŸ“– Consult docs for template activation requirements
```

---

## Formatting Rules

### Results Line Format

**Rule**: Only show counts when they're non-zero

```python
def format_results_line(passed: int, failed: int, skipped: int = 0) -> str:
    """Format results line with conditional display of counts.

    Examples:
        âœ… 17 passed                    # No failures
        âœ… 12 passed, âŒ 5 failed       # Some failures
        âœ… 10 passed, â­ï¸ 2 skipped     # Some skipped
        âœ… 10 passed, âŒ 3 failed, â­ï¸ 2 skipped  # All three
    """
    parts = [f"âœ… {passed} passed"]

    if failed > 0:
        parts.append(f"âŒ {failed} failed")

    if skipped > 0:
        parts.append(f"â­ï¸ {skipped} skipped")

    return "Results: " + ", ".join(parts)
```

### Tools vs Resources Format

**Consistent Structure**:

```text
ğŸ”§ TOOLS (selected/total tested, N skipped)
   Selection: [reason for filtering]
   Results: [formatted counts]

ğŸ—‚ï¸  RESOURCES (selected/total tested)
   Type Breakdown: [static vs templates]
   Results: [formatted counts]
```

**Key Points**:

- Tools always show "skipped" in header because test selection is policy-based
- Resources show "X/Y tested" format (no separate skipped, all resources run if configured)
- Both use same `format_results_line()` function for consistency

---

## Implementation Changes

### 1. Enhance `print_detailed_summary()` Signature

```python
def print_detailed_summary(
    tools_results: Optional[Dict[str, Any]] = None,
    resources_results: Optional[Dict[str, Any]] = None,
    selection_stats: Optional[Dict[str, Any]] = None,  # NEW
    server_info: Optional[Dict[str, Any]] = None,       # NEW
    verbose: bool = False                                # NEW
) -> None:
    """Print intelligent test summary with context.

    Args:
        tools_results: Tool test results from ToolsTester.to_dict()
        resources_results: Resource test results from ResourcesTester.to_dict()
        selection_stats: Stats from filter_tests_by_idempotence()
        server_info: Server capabilities from initialize()
        verbose: Include detailed configuration and analysis
    """
```

### 2. Pass Selection Stats Through Call Chain

```python
# In test_mcp.py main()
selection_stats = {
    'total_tools': 48,
    'selected_tools': 17,
    'effect_counts': {'none': 17, 'configure': 6, 'create': 15, 'remove': 5, 'update': 5},
    'total_resources': 24,
    'skipped_tools': 31
}

# In run_unified_tests()
success = MCPTester.run_test_suite(
    ...,
    selection_stats=selection_stats  # Pass through
)
```

### 3. Add Failure Classification

```python
class ResourceFailureType(enum.Enum):
    """Classify resource test failures for better reporting."""
    TEMPLATE_NOT_REGISTERED = "template_not_registered"
    URI_NOT_FOUND = "uri_not_found"
    CONTENT_VALIDATION = "content_validation"
    SERVER_ERROR = "server_error"
    CONFIG_ERROR = "config_error"

def classify_resource_failure(test_info: dict) -> ResourceFailureType:
    """Classify resource failure for intelligent reporting."""
    error = test_info.get('error', '')

    if 'Template not found in server resourceTemplates' in error:
        return ResourceFailureType.TEMPLATE_NOT_REGISTERED
    elif 'Resource not found in server resources' in error:
        return ResourceFailureType.URI_NOT_FOUND
    elif 'validation failed' in error.lower():
        return ResourceFailureType.CONTENT_VALIDATION
    elif 'error_type' in test_info and test_info['error_type'] == 'ConfigurationError':
        return ResourceFailureType.CONFIG_ERROR
    else:
        return ResourceFailureType.SERVER_ERROR
```

### 4. Add Pattern Analysis

```python
def analyze_failure_patterns(failed_tests: List[Dict]) -> Dict[str, Any]:
    """Analyze failure patterns to provide actionable insights.

    Returns:
        {
            'dominant_pattern': ResourceFailureType,
            'pattern_count': int,
            'total_failures': int,
            'recommendations': List[str],
            'severity': 'critical' | 'warning' | 'info'
        }
    """
    if not failed_tests:
        return {'severity': 'info', 'recommendations': []}

    # Classify all failures
    classifications = [classify_resource_failure(t) for t in failed_tests]

    # Find dominant pattern
    from collections import Counter
    pattern_counts = Counter(classifications)
    dominant = pattern_counts.most_common(1)[0]

    # Generate recommendations based on pattern
    recommendations = []
    severity = 'warning'

    if dominant[0] == ResourceFailureType.TEMPLATE_NOT_REGISTERED:
        if dominant[1] == len(failed_tests):
            # ALL failures are template registration
            severity = 'warning'  # Not critical - static resources work
            recommendations = [
                "âœ… Static resources all work - core MCP protocol OK",
                "ğŸ” Check server logs for template registration messages",
                "ğŸ”§ Review feature flags in config (SSO_ENABLED, ADMIN_API_ENABLED, etc.)",
                "ğŸ“– Consult docs for template activation requirements"
            ]
        else:
            severity = 'warning'
            recommendations = [
                "Some templates not registered - may need configuration",
                "Compare working vs failing templates for patterns"
            ]
    elif dominant[0] == ResourceFailureType.SERVER_ERROR:
        severity = 'critical'
        recommendations = [
            "âŒ Server errors detected - check server logs",
            "ğŸ› May indicate bugs in resource handlers",
            "ğŸ”§ Verify server is properly configured"
        ]

    return {
        'dominant_pattern': dominant[0],
        'pattern_count': dominant[1],
        'total_failures': len(failed_tests),
        'recommendations': recommendations,
        'severity': severity
    }
```

### 5. Update Overall Status Logic

```python
def determine_overall_status(tools_results, resources_results, analysis):
    """Determine overall test status with nuance."""
    tools_ok = not tools_results or tools_results['failed'] == 0
    resources_ok = not resources_results or resources_results['failed'] == 0

    if tools_ok and resources_ok:
        return "âœ… ALL TESTS PASSED"

    if not tools_ok:
        return "âŒ CRITICAL FAILURE"  # Tool failures are always critical

    # Tools passed, some resources failed - check pattern
    if analysis['severity'] == 'warning':
        return "âš ï¸  PARTIAL PASS"
    else:
        return "âŒ FAILURE"
```

---

## Output Examples

### Example 1: Default Mode, Template Issues (Current Scenario)

```
================================================================================
ğŸ“Š TEST SUITE SUMMARY
================================================================================

ğŸ”§ TOOLS (17/48 tested, 31 skipped)
   Selection: Idempotent only (31 non-idempotent skipped)
   Results: âœ… 17 passed

ğŸ—‚ï¸  RESOURCES (24/24 tested)
   Type Breakdown: 17 static URIs, 7 templates
   Results: âœ… 17 passed, âŒ 7 failed

   âš ï¸  All 7 failures: Template registration issues
      Templates not registered by server:
      - permissions://buckets/{bucket}/access
      - admin://users/{name}
      - athena://databases/{database}/tables
      - athena://databases/{database}/tables/{table}/schema
      - metadata://templates/{name}
      - workflow://workflows/{id}
      - tabulator://buckets/{bucket}/tables

   ğŸ“‹ Likely Causes:
      â€¢ Features require activation (env vars, feature flags)
      â€¢ Dynamic registration based on runtime config
      â€¢ Expected behavior for optional features

   ğŸ“Š Impact Assessment:
      âœ… Core MCP protocol working (all static resources pass)
      âœ… All idempotent tools working
      âš ï¸  Some advanced features unavailable

================================================================================
   Overall Status: âš ï¸  PARTIAL PASS
   - Core functionality verified (17/17 tools, 17/17 static resources)
   - 7 optional templates not registered (may be expected)
   - No critical failures detected
================================================================================

ğŸ’¡ Next Steps:
   â€¢ Review server logs for feature initialization messages
   â€¢ Check environment variables for feature flags
   â€¢ Run with --all to test write operations
   â€¢ Run with --verbose for detailed analysis
```

### Example 2: All Tests Pass

```
================================================================================
ğŸ“Š TEST SUITE SUMMARY
================================================================================

ğŸ”§ TOOLS (17/48 tested, 31 skipped)
   Selection: Idempotent only
   Results: âœ… 17 passed

ğŸ—‚ï¸  RESOURCES (24/24 tested)
   Results: âœ… 24 passed

================================================================================
   Overall Status: âœ… ALL TESTS PASSED
   - 17 idempotent tools verified
   - 24 resources verified (17 static, 7 templates)
   - No failures detected
================================================================================

ğŸ’¡ Run with --all to test write operations
```

### Example 3: Critical Tool Failures

```
================================================================================
ğŸ“Š TEST SUITE SUMMARY
================================================================================

ğŸ”§ TOOLS (17/48 tested, 31 skipped)
   Results: âœ… 12 passed, âŒ 5 failed

   âŒ Failed Tools (5):
      â€¢ bucket_objects_list: Connection timeout
      â€¢ bucket_object_info: Connection timeout
      â€¢ search_catalog: Connection timeout
      â€¢ package_browse: Connection timeout
      â€¢ athena_query_execute: Connection timeout

   ğŸ“‹ Failure Pattern Analysis:
      All 5 failures: Connection timeout
      Root Cause: Cannot reach AWS services

   ğŸ“Š Recommended Actions:
      1. âŒ Check network connectivity
      2. ğŸ”‘ Verify AWS credentials are configured
      3. ğŸŒ Check if running behind VPN/proxy
      4. ğŸ”§ Review security groups and network ACLs

================================================================================
   Overall Status: âŒ CRITICAL FAILURE
   - 5/17 core tools failing with connection issues
   - Server operational but cannot reach dependencies
   - Immediate action required
================================================================================
```

---

## Benefits

1. **No More Contradictions**: Clear distinction between "not tested" and "tested and failed"
2. **Context Preserved**: Final summary includes selection stats, no need to scroll
3. **Actionable Insights**: Pattern analysis suggests concrete next steps
4. **Appropriate Severity**: Distinguishes critical vs warning vs expected failures
5. **Progressive Detail**: Three verbosity levels for different use cases
6. **Pattern Recognition**: Automatic identification of common failure modes

---

## Implementation Priority

1. **Phase 1** (Critical): Add selection stats to final summary
2. **Phase 2** (High): Implement failure classification and pattern analysis
3. **Phase 3** (Medium): Add verbose mode with detailed breakdown
4. **Phase 4** (Low): Add debug mode with per-test details

---

## Migration Path

Existing code structure stays the same:

- `TestResults` class already ensures complete result dictionaries âœ…
- `print_detailed_summary()` is already the centralized output function âœ…
- Just need to enhance `print_detailed_summary()` with new features

Changes required:

1. Add `selection_stats` parameter to call chain
2. Add failure classification function
3. Add pattern analysis function
4. Update summary formatting in `print_detailed_summary()`
5. Add verbosity level support

All changes are **additive** - no breaking changes to existing tests or callers.
