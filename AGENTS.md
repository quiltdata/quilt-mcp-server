# Agent Guidelines: Quilt MCP Server

## Core Principle: Action Over Explanation

When users ask to "query", "understand", "explore", or "analyze" data, **execute tools to retrieve and analyze the data**. Do not provide code examples or theoretical responses when actual data can be retrieved using available MCP tools.

---

## Data Exploration Behavior

### âœ… CORRECT: Action-Oriented Response

**User:** "Can you provide a query so we can understand the content of these files?"

**Agent Response:**
```
Let me explore those files for you.

[Calls bucket_objects_list to find files]
[Calls bucket_object_info to get metadata]
[Calls bucket_object_text on readable files to sample contents]

I found 3 CSV files in that location. Let me show you what's in each:

File 1: experiment_data.csv
- Columns: sample_id, treatment, measurement_value, timestamp
- 1,250 rows
- Sample data: [shows first few rows]

File 2: metadata.csv  
- Columns: sample_id, patient_age, condition
- 800 rows
- Sample data: [shows first few rows]

Would you like me to create visualizations of this data? I can make:
1. Box plot of measurement_value by treatment
2. Distribution histogram of patient_age
3. Combined analysis showing relationships

Just let me know which you'd prefer, or I can create all of them.
```

### âŒ INCORRECT: Code-Focused Response

**User:** "Can you provide a query so we can understand the content of these files?"

**Agent Response:**
```
Perfect! I can now provide you with a comprehensive query to explore the files. 
Here's a Python script you can use to understand what's inside these files:

import pandas as pd

# Example: Load the CSV file
df = pd.read_csv('s3://bucket/file.csv')

# === BASIC FILE STRUCTURE ===
print("=" * 50)
print("BASIC STRUCTURE")
print("=" * 50)
print(f"Number of rows: {len(df)}")
...
[50 more lines of Python code]
```

**Why this is wrong:**
- âŒ Provides code instead of executing available tools
- âŒ Requires user to run code themselves
- âŒ Doesn't actually answer the question about file contents
- âŒ Wastes time with theoretical examples

---

## Tool Execution Guidelines

### When to Use Which Tools

| User Intent | Tool to Execute | What to Return |
|-------------|----------------|----------------|
| "What files are in...?" | `bucket_objects_list()` | Actual list of files with types/sizes |
| "Show me what's in this CSV" | `bucket_object_text()` | Parsed data with schema summary |
| "What's the structure of...?" | `bucket_object_info()` + `bucket_object_text()` | Metadata + content sample |
| "Create a visualization of..." | `create_data_visualization()` | Generated visualization config + files |
| "Package these files" | `package_create()` | Created package with catalog URL |
| "Query this data" | `bucket_object_text()` â†’ parse â†’ summarize | Actual data analysis results |

### Specialized File Formats

For formats requiring specific libraries (H5AD, Parquet, BAM, VCF, etc.):

1. **Acknowledge the format**: 
   ```
   "I found 5 H5AD files which contain single-cell RNA-seq data in AnnData format."
   ```

2. **Get what you can**:
   ```python
   # Call bucket_object_info() for each file
   ```
   
3. **Explain limitations clearly**:
   ```
   "H5AD files require the scanpy/anndata library to fully read their contents. 
   I can provide:
   - File metadata (size: 2.3 GB, modified: 2024-10-13)
   - Presigned download URLs for local analysis
   - Instructions for reading with scanpy if you have Python
   ```

4. **Offer actionable next steps**:
   ```
   Would you like me to:
   1. Generate presigned URLs so you can download these files?
   2. Look for any associated CSV or summary files that I can read?
   3. Create a package with these files and documentation about the format?
   ```

---

## Visualization Workflow

### Complete Workflow Example

**User:** "Create a visualization of gene expression data in s3://genomics-bucket/data/expression.csv"

**Correct Agent Behavior:**

```
Step 1: Reading the data file
[Calls bucket_object_text("s3://genomics-bucket/data/expression.csv")]

Found expression data with:
- Columns: gene_name, sample_id, expression_value, condition
- 5,000 rows (10 genes x 500 samples)
- Conditions: control, treated

Step 2: Creating visualization
[Calls create_data_visualization with:
  data=<parsed_csv>,
  plot_type="boxplot",
  x_column="gene_name",
  y_column="expression_value",
  group_column="condition",
  color_scheme="genomics"
]

Generated 3 files:
- gene_expression_boxplot.json (ECharts config)
- viz_data_boxplot.csv (processed data)
- quilt_summarize.json (package metadata)

Step 3: Uploading visualization files
[Calls bucket_objects_put with all files]

Step 4: Creating package
[Calls package_create with:
  name="genomics/expression-analysis",
  s3_uris=[original CSV + visualization files],
  registry="s3://genomics-bucket"
]

âœ… Complete! View your package here:
https://catalog.example.com/b/genomics-bucket/packages/genomics/expression-analysis/

The visualization is now live in the Quilt catalog and will display automatically
when viewing the package.
```

---

## Common Anti-Patterns to Avoid

### 1. âŒ Providing Code Instead of Executing

**Wrong:**
```
Here's how you can read the file:
```python
import scanpy as sc
adata = sc.read_h5ad('file.h5ad')
```
**Right:**
```
Let me read that file for you...
[Executes appropriate tool]
Here's what I found: ...
```

### 2. âŒ Suggesting Without Doing

**Wrong:**
```
You could use bucket_objects_list() to see what files are there.
```

**Right:**
```
Let me list those files for you...
[Calls bucket_objects_list()]
Found 15 files: ...
```

### 3. âŒ Asking Permission for Standard Operations

**Wrong:**
```
Would you like me to read the CSV file to see what's inside?
```

**Right:**
```
Reading the CSV file to show you the structure...
[Executes tool immediately]
```

### 4. âŒ Incomplete Workflows

**Wrong:**
```
I can create a visualization. Just provide the data in this format: ...
```

**Right:**
```
[Gets the data using available tools]
[Creates visualization with actual data]
[Completes full workflow]
Here's your visualization: ...
```

---

## Error Handling and User Feedback

### When Operations Fail

**Good Error Handling:**
```
I tried to read the file but encountered an access error. Let me check your permissions...
[Calls bucket_access_check()]

You have read access to the bucket but not this specific file prefix.
Here's what we can do:
1. Try a different file in the bucket
2. Request access to this prefix
3. Work with publicly accessible files instead

Which would you prefer?
```

**Poor Error Handling:**
```
Error: Access Denied. You need to fix your IAM permissions.
```

### Progress Updates for Long Operations

For operations that take time:
```
Working on creating your package with 500 files...
âœ… Organized files by type (2s)
âœ… Generated visualizations (5s)
âœ… Created package structure (3s)
ğŸ”„ Uploading to S3... (15s)
âœ… Complete! Package created: [URL]
```

---

## Testing Your Behavior

### Self-Check Questions

Before responding to a user query, ask yourself:

1. **Am I executing tools or just explaining?**
   - âœ… Execute â†’ Show results
   - âŒ Explain â†’ Show code

2. **Can I get actual data right now?**
   - âœ… Yes â†’ Get it and show it
   - âŒ No â†’ Explain why and offer alternatives

3. **Am I completing the full workflow?**
   - âœ… End-to-end completion
   - âŒ Partial steps requiring user action

4. **Am I using available tools effectively?**
   - âœ… Chaining tools together for complete results
   - âŒ Using only one tool when more are needed

---

## Quick Reference: User Intent â†’ Agent Action

| User Says | Agent Does |
|-----------|------------|
| "What's in these files?" | `bucket_objects_list` + `bucket_object_text` â†’ Show actual content |
| "Query this data" | Read file + Parse + Analyze â†’ Show results |
| "Understand this" | Execute appropriate tools â†’ Show findings |
| "Create visualization" | Read data + Generate viz + Upload + Package â†’ Show URL |
| "Make a package" | Organize + Create + Verify â†’ Show catalog link |
| "Explore this bucket" | List + Sample files + Summarize â†’ Show overview |

---

## Python Execution Guidelines

### Always Use 'uv run' for Python Scripts and Tests

**CRITICAL:** This project uses `uv` as the Python package manager. When executing Python scripts or tests, always use `uv run` prefix:

**âœ… CORRECT:**
```bash
uv run python scripts/test-mcp-tool-call-formats.py
uv run pytest tests/unit/
uv run python -m pytest tests/integration/
uv run mypy src/
```

**âŒ INCORRECT:**
```bash
python scripts/test-mcp-tool-call-formats.py
pytest tests/unit/
python -m pytest tests/integration/
mypy src/
```

**Why this matters:**
- Ensures correct virtual environment activation
- Uses project-specific dependencies from `uv.lock`
- Prevents import errors and version conflicts
- Maintains consistency with project setup

**Exception:** Only use direct `python` commands when specifically working outside the project environment or when `uv run` is not available.

---

## Project Structure

### Source Code Organization (`src/quilt_mcp/`)

```text
src/quilt_mcp/
â”œâ”€â”€ backends/           # Backend implementations (Quilt3, QuiltOps)
â”‚   â””â”€â”€ quilt3_backend.py
â”œâ”€â”€ cli/               # Command-line interface
â”œâ”€â”€ context/           # Request context management
â”‚   â”œâ”€â”€ handler.py
â”‚   â”œâ”€â”€ factory.py
â”‚   â”œâ”€â”€ request_context.py
â”‚   â”œâ”€â”€ tenant_extraction.py
â”‚   â”œâ”€â”€ propagation.py
â”‚   â””â”€â”€ exceptions.py
â”œâ”€â”€ domain/            # Domain objects (QuiltOps migration)
â”œâ”€â”€ middleware/        # HTTP middleware (JWT auth)
â”‚   â””â”€â”€ jwt_middleware.py
â”œâ”€â”€ models/            # Data models and schemas
â”œâ”€â”€ ops/               # QuiltOps abstraction layer
â”œâ”€â”€ optimization/      # Performance optimization
â”‚   â”œâ”€â”€ autonomous.py
â”‚   â”œâ”€â”€ interceptor.py
â”‚   â”œâ”€â”€ integration.py
â”‚   â”œâ”€â”€ scenarios.py
â”‚   â””â”€â”€ testing.py
â”œâ”€â”€ search/            # Search functionality
â”œâ”€â”€ services/          # Business logic services
â”œâ”€â”€ storage/           # Data persistence
â”œâ”€â”€ telemetry/         # Observability and metrics
â”œâ”€â”€ tools/             # MCP tool implementations
â”‚   â”œâ”€â”€ auth_helpers.py
â”‚   â”œâ”€â”€ buckets.py
â”‚   â”œâ”€â”€ catalog.py
â”‚   â”œâ”€â”€ data_visualization.py
â”‚   â”œâ”€â”€ error_recovery.py
â”‚   â”œâ”€â”€ packages.py
â”‚   â”œâ”€â”€ quilt_summary.py
â”‚   â”œâ”€â”€ resource_access.py
â”‚   â”œâ”€â”€ search.py
â”‚   â””â”€â”€ stack_buckets.py
â”œâ”€â”€ validators/        # Input validation
â”œâ”€â”€ visualization/     # Data visualization engine
â”‚   â”œâ”€â”€ analyzers/     # File and data analysis
â”‚   â”‚   â”œâ”€â”€ data_analyzer.py
â”‚   â”‚   â”œâ”€â”€ file_analyzer.py
â”‚   â”‚   â””â”€â”€ genomic_analyzer.py
â”‚   â”œâ”€â”€ generators/    # Visualization generators
â”‚   â”‚   â”œâ”€â”€ echarts.py
â”‚   â”‚   â”œâ”€â”€ igv.py
â”‚   â”‚   â”œâ”€â”€ matplotlib.py
â”‚   â”‚   â”œâ”€â”€ perspective.py
â”‚   â”‚   â””â”€â”€ vega_lite.py
â”‚   â”œâ”€â”€ layouts/       # Layout management
â”‚   â”‚   â””â”€â”€ grid_layout.py
â”‚   â”œâ”€â”€ utils/         # Visualization utilities
â”‚   â”‚   â”œâ”€â”€ data_processing.py
â”‚   â”‚   â””â”€â”€ file_utils.py
â”‚   â””â”€â”€ engine.py      # Main visualization engine
â”œâ”€â”€ config.py          # Configuration management
â”œâ”€â”€ constants.py       # Application constants
â”œâ”€â”€ formatting.py      # Output formatting
â”œâ”€â”€ health.py          # Health check endpoints
â”œâ”€â”€ main.py            # MCP server entry point
â”œâ”€â”€ resources.py       # Resource management
â”œâ”€â”€ runtime_context.py # Runtime context utilities
â”œâ”€â”€ utils.py           # General utilities
â””â”€â”€ version_sync.py    # Version synchronization
```

### Test Organization (`tests/`)

```text
tests/
â”œâ”€â”€ unit/              # Unit tests (isolated component testing)
â”‚   â”œâ”€â”€ backends/      # Backend implementation tests
â”‚   â”œâ”€â”€ context/       # Context management tests
â”‚   â”œâ”€â”€ domain/        # Domain object tests
â”‚   â”œâ”€â”€ models/        # Model tests
â”‚   â”œâ”€â”€ ops/           # QuiltOps tests
â”‚   â”œâ”€â”€ resources/     # Resource tests
â”‚   â”œâ”€â”€ search/        # Search tests
â”‚   â”œâ”€â”€ server/        # Server tests
â”‚   â”œâ”€â”€ services/      # Service tests
â”‚   â”œâ”€â”€ storage/       # Storage tests
â”‚   â””â”€â”€ tools/         # Tool tests
â”œâ”€â”€ integration/       # Integration tests (multiple components)
â”œâ”€â”€ e2e/               # End-to-end tests (full workflows)
â”œâ”€â”€ stateless/         # Stateless operation tests
â”œâ”€â”€ security/          # Security and auth tests
â”œâ”€â”€ performance/       # Performance benchmarks
â”œâ”€â”€ load/              # Load testing
â”œâ”€â”€ fixtures/          # Test data and fixtures
â”œâ”€â”€ conftest.py        # Pytest configuration
â””â”€â”€ helpers.py         # Test utilities
```

### Key Architecture Patterns

1. **Backends**: Abstract interface for Quilt3 and QuiltOps implementations
2. **Context**: Request-scoped context propagation (tenant, auth, etc.)
3. **Domain**: Domain-driven design objects for QuiltOps migration
4. **Tools**: MCP tool definitions that expose functionality to AI agents
5. **Visualization**: Pluggable visualization engine with multiple generators
6. **Ops**: Abstraction layer for migrating from Quilt3 to QuiltOps

### Testing Strategy

- **Unit tests**: Test individual components in isolation with mocks
- **Integration tests**: Test multiple components working together
- **E2E tests**: Test complete user workflows from start to finish
- **Stateless tests**: Verify operations don't depend on external state
- **Security tests**: Test authentication, authorization, and data access
- **Performance tests**: Benchmark critical operations
- **Load tests**: Test system behavior under stress

---

## Summary

**Core Behavior:** When users want to understand, query, or explore data:
1. Execute tools immediately to retrieve actual data
2. Analyze and summarize the real data you retrieved
3. Offer concrete next steps based on what you found
4. Only provide code examples if tools aren't available

**Never:**
- Give theoretical responses when tools can execute
- Provide code for the user to run when you can run tools
- Suggest capabilities without demonstrating them
- Leave workflows incomplete
- Use bare `python` or `pytest` commands (always use `uv run`)

**Always:**
- Execute available tools proactively
- Show actual results from real data
- Complete full workflows end-to-end
- Offer specific, actionable next steps
- Use `uv run` prefix for all Python script and test execution
