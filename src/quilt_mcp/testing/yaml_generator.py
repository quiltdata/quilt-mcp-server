"""YAML test configuration generation.

This module generates comprehensive test configurations in YAML format by
combining tool discovery, argument inference, and tool loop generation.
It produces ready-to-use test configs that cover all registered MCP tools.

Core Functions
--------------
generate_test_yaml(server, output_file, env_vars, skip_discovery, discovery_timeout) -> None
    Generate complete mcp-test.yaml configuration by discovering tools,
    inferring arguments, generating loops, and validating coverage.

generate_csv_output(items, output_file) -> None
    Generate CSV format output for tools and resources, useful for
    spreadsheet analysis and reporting.

generate_json_output(items, output_file) -> None
    Generate structured JSON output for programmatic consumption,
    useful for tooling integration and automation.

Generation Process
------------------
1. Discovery Phase
   - List all registered tools and resources
   - Extract metadata (names, descriptions, schemas)
   - Classify by effect and category
   - Execute discovery to find real data

2. Inference Phase
   - Infer test arguments from signatures
   - Use discovered data (S3 keys, packages)
   - Apply environment variables
   - Generate validation rules

3. Loop Generation Phase
   - Generate create/modify/verify/cleanup loops
   - Cover all write-effect operations
   - Include rollback and error handling
   - Validate loop coverage

4. Validation Phase
   - Ensure all tools have test coverage
   - Validate loop coverage for write operations
   - Check configuration completeness
   - Verify no duplicate test names

5. Output Phase
   - Generate YAML with comments
   - Format for readability
   - Include usage instructions
   - Add configuration metadata

Generated Configuration Structure
---------------------------------
```yaml
# Generated by mcp-test-setup.py
# Generated at: 2024-02-08T10:30:00Z
# Server version: 0.1.0
# Discovery: enabled (15.0s timeout)

# Environment variables used for generation
environment:
  TEST_QUILT_CATALOG_URL: s3://my-bucket
  TEST_QUILT_CATALOG_BUCKET: my-bucket
  TEST_QUILT_ATHENA_DATABASE: my_db

# Tool tests (read operations)
tools:
  bucket_list:
    args:
      catalog_url: s3://my-bucket
    effect: none
    category: required-arg

  package_search:
    args:
      catalog_url: s3://my-bucket
      query: "*"
    effect: none
    category: required-arg
    search_validation:
      min_results: 1
      required_fields: [name, modified]

# Resource tests (access validation)
resources:
  s3://my-bucket/data.json:
    expected_mime_type: application/json
    description: "Discovered S3 object"

  quilt://my-bucket/my-package:
    expected_mime_type: application/json
    description: "Discovered Quilt package"

# Tool loops (write operations with cleanup)
tool_loops:
  package_lifecycle:
    create:
      tool: package_create
      args:
        name: test-package-{uuid}
        catalog_url: s3://my-bucket
      store_result: package_name

    verify:
      tool: package_list
      args:
        catalog_url: s3://my-bucket
      validate:
        contains: test-package-{uuid}

    cleanup:
      tool: package_delete
      args:
        name: "{stored.package_name}"
        catalog_url: s3://my-bucket
      ignore_errors: true

# Statistics
# Total tools: 42
# Tools with tests: 42
# Resources discovered: 15
# Tool loops: 8
```

CSV Output Format
-----------------
Columns:
- name: Tool or resource name
- type: "tool" or "resource"
- effect: Effect classification
- category: Category classification
- description: Human-readable description
- has_test: Boolean indicating test coverage

JSON Output Format
------------------
Structure:
```json
{
  "generated_at": "2024-02-08T10:30:00Z",
  "server_version": "0.1.0",
  "discovery_enabled": true,
  "tools": [
    {
      "name": "bucket_list",
      "type": "tool",
      "effect": "none",
      "category": "required-arg",
      "description": "List S3 buckets",
      "input_schema": {...},
      "has_test": true,
      "test_args": {...}
    }
  ],
  "resources": [...],
  "tool_loops": [...],
  "statistics": {
    "total_tools": 42,
    "tools_with_tests": 42,
    "resources_discovered": 15,
    "tool_loops": 8
  }
}
```

Usage Examples
--------------
Generate test YAML with discovery:
    >>> await generate_test_yaml(
    ...     server=mcp_server,
    ...     output_file="config/mcp-test.yaml",
    ...     env_vars={
    ...         "TEST_QUILT_CATALOG_URL": "s3://my-bucket",
    ...         "TEST_QUILT_CATALOG_BUCKET": "my-bucket"
    ...     },
    ...     skip_discovery=False,
    ...     discovery_timeout=15.0
    ... )
    Generated config/mcp-test.yaml with 42 tools, 15 resources, 8 loops

Generate without discovery (faster):
    >>> await generate_test_yaml(
    ...     server=mcp_server,
    ...     output_file="config/mcp-test.yaml",
    ...     env_vars=env,
    ...     skip_discovery=True
    ... )
    Generated config/mcp-test.yaml with 42 tools, 0 resources, 8 loops

Generate CSV for reporting:
    >>> tools = [
    ...     {"name": "bucket_list", "effect": "none", "description": "List buckets"},
    ...     {"name": "package_create", "effect": "create", "description": "Create package"}
    ... ]
    >>> generate_csv_output(tools, "reports/tools.csv")

Generate JSON for tooling:
    >>> generate_json_output(tools, "reports/tools.json")

Design Principles
-----------------
- Complete coverage by default
- Discoverable real data when possible
- Sensible fallbacks for environment
- Readable YAML with comments
- Validation before output
- Statistics for transparency

Discovery Options
-----------------
1. Full Discovery (Default)
   - Execute all safe tools
   - Capture real infrastructure state
   - Generate realistic test data
   - 15-second timeout per tool

2. Skip Discovery (--skip-discovery)
   - Faster generation
   - Use only environment variables
   - No real data samples
   - Suitable for CI/CD

3. Timeout Configuration
   - Configurable per-tool timeout
   - Default 15 seconds
   - Prevents hanging on slow operations
   - Logs timeout events

Error Handling
--------------
The generator handles multiple failure scenarios:

1. Discovery Failures
   - Log and continue with partial data
   - Use environment fallbacks
   - Mark tools as untested if needed

2. Inference Failures
   - Skip problematic tools
   - Log warnings
   - Continue with remaining tools

3. Validation Failures
   - Report coverage gaps
   - Suggest manual additions
   - Don't fail generation

4. I/O Failures
   - Clear error messages
   - Validate output path writability
   - Atomic file writes

Dependencies
------------
- tool_classifier.py: classify_tool, infer_arguments
- tool_loops.py: generate_tool_loops
- validators.py: validate_test_coverage
- discovery.py: DiscoveryOrchestrator
- csv, json, yaml: Output formatting
- pathlib: File handling

Extracted From
--------------
- generate_csv_output: lines 1190-1208 from scripts/mcp-test-setup.py
- generate_json_output: lines 1210-1228 from scripts/mcp-test-setup.py
- generate_test_yaml: lines 1266-1743 from scripts/mcp-test-setup.py
"""

import csv
import inspect
import json
import re
from typing import Any, Dict, List, Optional

import yaml

from .config import truncate_response
from .discovery import DiscoveryOrchestrator
from .tool_classifier import classify_tool, get_user_athena_database, infer_arguments
from .tool_loops import generate_tool_loops, get_test_roles, validate_tool_loops_coverage


def generate_csv_output(items: List[Dict[str, Any]], output_file: str):
    """Generate CSV output for tools and resources with type column.

    Args:
        items: List of tool/resource metadata dictionaries
        output_file: Path to output CSV file

    CSV Columns:
        - type: "tool" or "resource"
        - module: Module name
        - function_name: Item name
        - signature: Function signature
        - description: Description text
        - is_async: Whether function is async
        - full_module_path: Full module path
    """
    with open(output_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(
            ["type", "module", "function_name", "signature", "description", "is_async", "full_module_path"]
        )

        for item in items:
            writer.writerow(
                [
                    item["type"],
                    item["module"],
                    item["name"],
                    item["signature"],
                    item["description"],
                    str(item["is_async"]),
                    item["full_module_path"],
                ]
            )


def generate_json_output(items: List[Dict[str, Any]], output_file: str):
    """Generate structured JSON output for tooling.

    Args:
        items: List of tool/resource metadata dictionaries
        output_file: Path to output JSON file

    JSON Structure:
        {
            "metadata": {...},
            "tools": [...],
            "resources": [...]
        }
    """
    tools = [item for item in items if item["type"] == "tool"]
    resources = [item for item in items if item["type"] == "resource"]

    output = {
        "metadata": {
            "generated_by": "scripts/mcp-test-setup.py",
            "tool_count": len(tools),
            "resource_count": len(resources),
            "total_count": len(items),
            "modules": list(set(item["module"] for item in items)),
        },
        "tools": tools,
        "resources": resources,
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)


async def generate_test_yaml(
    server,
    output_file: str,
    env_vars: Dict[str, str | None],
    skip_discovery: bool = False,
    discovery_timeout: float = 15.0,
):
    """Generate mcp-test.yaml configuration with all available tools and resources.

    This creates test configurations for mcp-test.py to validate the MCP server.
    Each tool gets a basic test case that can be customized as needed.
    Each resource gets test configuration with URI patterns and validation rules.
    Environment configuration from .env is embedded for self-contained testing.

    Phase 2 Enhancement: Runs discovery to validate tools and capture real data.

    Phase 4 Enhancement (A18): Generates tool_loops section for write-operation testing.

    Args:
        server: MCP server instance
        output_file: Path to output YAML file
        env_vars: Environment variables from .env
        skip_discovery: Skip tool discovery phase
        discovery_timeout: Timeout in seconds for each tool discovery
    """
    # Extract test-relevant configuration from environment
    test_config = {
        "_generated_by": "scripts/mcp-test-setup.py - Auto-generated test configuration with discovery and tool loops",
        "_note": "Edit test cases below to customize arguments and validation. Tool loops test write operations.",
        "environment": {
            "AWS_PROFILE": env_vars.get("AWS_PROFILE", "default"),
            "AWS_DEFAULT_REGION": env_vars.get("AWS_DEFAULT_REGION", "us-east-1"),
            "QUILT_CATALOG_URL": env_vars.get("QUILT_CATALOG_URL", ""),
            "QUILT_TEST_BUCKET": env_vars.get("QUILT_TEST_BUCKET", ""),
            "QUILT_TEST_PACKAGE": env_vars.get("QUILT_TEST_PACKAGE", ""),
            "QUILT_TEST_ENTRY": env_vars.get("QUILT_TEST_ENTRY", ""),
        },
        "test_tools": {},
        "test_resources": {},
        "tool_loops": {},  # NEW: Tool loops for write-operation testing
        "test_config": {"timeout": 30, "retry_attempts": 2, "fail_fast": False},
    }

    # Initialize discovery orchestrator
    orchestrator = DiscoveryOrchestrator(server, timeout=discovery_timeout, verbose=True, env_vars=env_vars)

    # Get all registered tools
    # Get all tools
    server_tools = await server.get_tools()

    # Load values from .env
    test_bucket: str = env_vars.get("QUILT_TEST_BUCKET") or "s3://quilt-example"
    catalog_url: str = env_vars.get("QUILT_CATALOG_URL") or "https://open.quiltdata.com"
    test_package: str = env_vars.get("QUILT_TEST_PACKAGE") or "examples/wellplates"
    test_entry: str = env_vars.get("QUILT_TEST_ENTRY") or ".timestamp"
    bucket_name = test_bucket.replace("s3://", "").split("/")[0]

    # Get UserAthenaDatabase from the CloudFormation stack
    print(f"\nüîç Looking up UserAthenaDatabase from stack for catalog: {catalog_url}")
    athena_database = get_user_athena_database(catalog_url)

    # Auto-generate tool order from all discovered tools
    # Special case: bucket_objects_list runs FIRST to discover real objects
    all_tool_names = list(server_tools.keys())

    # Separate bucket_objects_list from others
    priority_tools = []
    if "bucket_objects_list" in all_tool_names:
        priority_tools.append("bucket_objects_list")
        all_tool_names.remove("bucket_objects_list")

    # Sort remaining tools alphabetically for deterministic ordering
    all_tool_names.sort()

    # Final order: priority tools first, then all others
    tool_order = priority_tools + all_tool_names

    # Tools with multiple test variants based on parameter combinations
    # Format: tool_name -> {"param_name": [test_value1, test_value2, ...]}
    # Special handling: if variant name contains "package", uses QUILT_TEST_PACKAGE, else QUILT_TEST_ENTRY
    # For search_catalog: Only test without bucket (wildcard across all buckets)
    # Note: Removed "with_bucket" mode as hardcoded bucket assumptions are being eliminated
    tool_variants = {
        "search_catalog": {
            "scope": ["global", "file", "package"],
            "bucket_mode": ["no_bucket"],  # Test wildcard patterns only
        }
    }

    # Custom test configurations for specific tools
    # For tools with variants, use tool_name.variant_value format
    # Note: Empty dict {} means tool has no required params (will be auto-filled by effect classifier)
    custom_configs = {
        # Catalog operations
        "catalog_configure": {"catalog_url": catalog_url},
        "catalog_uri": {"registry": test_bucket, "package_name": test_package, "path": ".timestamp"},
        "catalog_url": {"registry": test_bucket, "package_name": test_package, "path": ".timestamp"},
        # Bucket operations (discovery)
        "bucket_objects_list": {"bucket": bucket_name, "prefix": f"{test_package}/", "max_keys": 5},
        "bucket_object_info": {"s3_uri": f"s3://{test_bucket}/{test_package}/.timestamp"},
        "bucket_object_link": {"s3_uri": f"s3://{test_bucket}/{test_package}/.timestamp"},
        "bucket_object_text": {"s3_uri": f"s3://{test_bucket}/{test_package}/.timestamp", "max_bytes": 200},
        "bucket_object_fetch": {"s3_uri": f"s3://{test_bucket}/{test_package}/.timestamp", "max_bytes": 200},
        # bucket_objects_put: Omitted - will be tested via tool loops
        # Package operations (read-only)
        "package_browse": {
            "package_name": test_package,
            "registry": test_bucket,
            "recursive": False,
            "include_signed_urls": False,
            "top": 5,
        },
        "package_diff": {"package1_name": test_package, "package2_name": test_package, "registry": test_bucket},
        # package_create, package_update, package_delete: Omitted - will be tested via tool loops
        # Search operations (search_catalog variants auto-generated, see tool_variants)
        "search_explain": {"query": "CSV files"},
        "search_suggest": {"partial_query": test_package[:5], "limit": 5},
        # Query operations
        "athena_query_validate": {"query": "SHOW TABLES"},
        "athena_query_execute": {"query": "SELECT 1 as test_value", "max_results": 10},
        "athena_tables_list": {"database": athena_database},
        # Use a known table that exists in the UserAthenaDatabase
        "athena_table_schema": {"database": athena_database, "table": "ai2-semanticscholar-cord-19_manifests"},
        "tabulator_bucket_query": {"bucket_name": bucket_name, "query": "SELECT 1 as test_value", "max_results": 10},
        "tabulator_tables_list": {"bucket": bucket_name},
        "tabulator_open_query_status": {},
        # tabulator_open_query_toggle: Omitted - will be tested via tool loops
        # Workflow operations
        "workflow_template_apply": {
            "template_name": "cross-package-aggregation",
            "workflow_id": "test-wf-001",
            "params": {"source_packages": [test_package], "target_package": f"{test_package}-agg"},
        },
        # workflow_create, workflow_add_step, workflow_update_step: Omitted - will be tested via tool loops
        # Visualization operations
        # create_data_visualization: Omitted - will be tested via tool loops
        # Permissions operations - limit to test bucket for faster execution
        "discover_permissions": {"check_buckets": [bucket_name]},
        "check_bucket_access": {"bucket": bucket_name},
        # Admin/Governance operations (read-only ones included for testing)
        "admin_user_get": {"name": "admin"},
        # admin_user_create, admin_user_delete, etc.: Omitted - will be tested via tool loops
        # admin_sso_*, admin_tabulator_*: Omitted - will be tested via tool loops
    }

    # Process tools in defined order
    for tool_name in tool_order:
        if tool_name not in server_tools:
            continue

        handler = server_tools[tool_name]
        doc = inspect.getdoc(handler.fn) or "No description available"

        # Classify tool
        effect, category = classify_tool(tool_name, handler)

        # Check if this tool has variants
        if tool_name in tool_variants:
            # Generate test cases for each variant combination
            variants_config = tool_variants[tool_name]

            # Handle multi-dimensional variants (e.g., scope x bucket_mode)
            if "bucket_mode" in variants_config:
                # Special handling for search_catalog with scope and bucket combinations
                scope_values = variants_config.get("scope", ["global"])
                bucket_modes = variants_config.get("bucket_mode", ["with_bucket"])

                for scope in scope_values:
                    for bucket_mode in bucket_modes:
                        # Create variant key like "search_catalog.file.no_bucket"
                        variant_key = f"{tool_name}.{scope}.{bucket_mode}"

                        # Determine query value based on scope
                        query_value = test_package if scope == "package" else test_entry

                        # Build arguments based on bucket_mode
                        arguments = {"query": query_value, "limit": 10, "scope": scope}

                        if bucket_mode == "with_bucket":
                            arguments["bucket"] = test_bucket
                        # else: no_bucket - omit bucket parameter to test search across all indexes

                        test_case = {
                            "tool": tool_name,  # Store the actual tool name
                            "description": doc.split('\n')[0],
                            "effect": effect,
                            "arguments": arguments,
                            "response_schema": {
                                "type": "object",
                                "properties": {"content": {"type": "array", "items": {"type": "object"}}},
                                "required": ["content"],
                            },
                        }

                        # Add smart validation rules for search variants
                        # For "no_bucket" mode, allow 0 results since elasticsearch may not be
                        # available in test environments (Docker containers).
                        # For "with_bucket" mode, require at least 1 result to verify search works.
                        validation = {
                            "type": "search",
                            "min_results": 0 if bucket_mode == "no_bucket" else 1,
                            "must_contain": [],
                        }

                        if scope == "file":
                            # File search must find TEST_ENTRY
                            if bucket_mode == "with_bucket":
                                validation["description"] = (
                                    f"File search with specific bucket must find TEST_ENTRY ({test_entry})"
                                )
                                validation["must_contain"].append(
                                    {
                                        "value": test_entry,
                                        "field": "title",
                                        "match_type": "substring",
                                        "description": f"Must find {test_entry} in file search results (title field)",
                                    }
                                )
                            else:
                                validation["description"] = (
                                    "File search across all buckets (may return 0 if bucket enumeration unavailable)"
                                )

                            validation["result_shape"] = {"required_fields": ["id", "type", "title", "score"]}

                        elif scope == "package":
                            # Package search should return results
                            if bucket_mode == "with_bucket":
                                validation["description"] = "Package search with specific bucket should return results"
                                validation["min_results"] = 1
                            else:
                                validation["description"] = (
                                    "Package search across all buckets (may return 0 if bucket enumeration unavailable)"
                                )

                            validation["result_shape"] = {"required_fields": ["id", "type", "score"]}

                        elif scope == "global":
                            # Global search should find test entry
                            if bucket_mode == "with_bucket":
                                validation["description"] = (
                                    "Global search with specific bucket should return results including test entry"
                                )
                                validation["must_contain"].append(
                                    {
                                        "value": test_entry,
                                        "field": "title",
                                        "match_type": "substring",
                                        "description": f"Must find TEST_ENTRY ({test_entry}) in global results (title field)",
                                    }
                                )
                                validation["min_results"] = 1
                            else:
                                validation["description"] = (
                                    "Global search across all buckets (may return 0 if bucket enumeration unavailable)"
                                )

                        test_case["validation"] = validation
                        test_config["test_tools"][variant_key] = test_case
            else:
                # Legacy single-parameter variant handling (for future tools if needed)
                for param_name, param_values in variants_config.items():
                    for param_value in param_values:
                        variant_key = f"{tool_name}.{param_value}"
                        arguments = {"query": test_entry, "limit": 10, param_name: param_value}
                        test_case = {
                            "tool": tool_name,
                            "description": doc.split('\n')[0],
                            "effect": effect,
                            "arguments": arguments,
                            "response_schema": {
                                "type": "object",
                                "properties": {"content": {"type": "array", "items": {"type": "object"}}},
                                "required": ["content"],
                            },
                        }
                        test_config["test_tools"][variant_key] = test_case
        else:
            # Single test case for tools without variants
            # Use custom config if available, otherwise infer arguments
            if tool_name in custom_configs:
                arguments = custom_configs[tool_name]
            else:
                # Infer arguments from signature and environment
                arguments = infer_arguments(
                    tool_name, handler, env_vars, orchestrator.registry.to_dict(), athena_database
                )

            test_case = {
                "description": doc.split('\n')[0],
                "effect": effect,
                "category": category,  # Track tool category
                "arguments": arguments,
                "response_schema": {
                    "type": "object",
                    "properties": {"content": {"type": "array", "items": {"type": "object"}}},
                    "required": ["content"],
                },
            }

            test_config["test_tools"][tool_name] = test_case

    # ========================================================================
    # Phase 2: Discovery - Execute tools and validate
    # ========================================================================
    if not skip_discovery:
        print("\nüîç Phase 2: Discovering & Validating Tools...")
        print(f"   Running discovery for {len(test_config['test_tools'])} tool configurations...")

        discovery_count = 0
        for test_key, test_case in test_config["test_tools"].items():
            # Get the actual tool name (may differ from test_key for variants)
            actual_tool_name = test_case.get("tool", test_key)

            # Skip if not in server_tools
            if actual_tool_name not in server_tools:
                continue

            handler = server_tools[actual_tool_name]
            arguments = test_case.get("arguments", {})
            effect = test_case.get("effect", "none")
            category = test_case.get("category", "required-arg")

            # Run discovery
            result = await orchestrator.discover_tool(actual_tool_name, handler, arguments, effect, category)

            # Store result
            orchestrator.results[test_key] = result

            # Add discovery info to test case
            # NOTE: Omit volatile data (precise timings, timestamps, presigned URLs)
            # to avoid unnecessary git diffs on every run
            discovery_info = {
                "status": result.status,
                # Round duration to nearest 100ms to reduce noise
                "duration_ms": round(result.duration_ms / 100) * 100 if result.duration_ms else 0,
            }

            if result.status == 'PASSED':
                # Add response example (truncate if too large)
                if result.response:
                    truncated = truncate_response(result.response, max_size=1000)
                    discovery_info["response_example"] = truncated
                if result.discovered_data:
                    discovery_info["discovered_data"] = result.discovered_data

                discovery_count += 1
                if discovery_count <= 5 or orchestrator.verbose:  # Print first 5 or all if verbose
                    print(f"  ‚úì {test_key} ({result.duration_ms:.0f}ms)")

            elif result.status == 'FAILED':
                discovery_info["error"] = result.error
                discovery_info["error_category"] = result.error_category
                print(f"  ‚úó {test_key}: {result.error}")

            elif result.status == 'SKIPPED':
                if discovery_count <= 3:  # Only print first few skipped
                    print(f"  ‚äò {test_key}: {result.error}")

            test_case["discovery"] = discovery_info

        # Print summary
        orchestrator.print_summary()

        # Add discovered data registry to config
        test_config["discovered_data"] = orchestrator.registry.to_dict()

    # ========================================================================
    # Phase 4: Generate Tool Loops
    # ========================================================================
    print("\nüîÑ Phase 4: Generating Tool Loops for Write Operations...")
    # Get standard test roles for tool loops
    base_role, secondary_role = get_test_roles()
    tool_loops = generate_tool_loops(env_vars, base_role, secondary_role)
    test_config["tool_loops"] = tool_loops
    print(f"   Generated {len(tool_loops)} tool loops")

    # Validate coverage
    print("\nüìä Validating Tool Coverage...")
    validate_tool_loops_coverage(server_tools, tool_loops, test_config["test_tools"])

    # Generate resource test configuration
    print("\nüóÇÔ∏è  Generating resource test configuration...")

    # Get resources from FastMCP server
    static_resources = await server.get_resources()
    resource_templates = await server.get_resource_templates()

    # Process both static resources and templates
    # Both are dicts with URI (template) keys and FunctionResource values
    all_resources = []
    for uri, resource in static_resources.items():
        # ERROR if resource lacks a description
        if not hasattr(resource, 'description') or not resource.description:
            raise ValueError(f"Resource '{uri}' is missing a description in test YAML generation!")
        all_resources.append((uri, resource.description))

    for uri_template, template in resource_templates.items():
        # ERROR if template lacks a description
        if not hasattr(template, 'description') or not template.description:
            raise ValueError(f"Resource template '{uri_template}' is missing a description in test YAML generation!")
        all_resources.append((uri_template, template.description))

    for uri_pattern, doc in all_resources:
        # Build basic test case structure - default to JSON since most resources return JSON
        test_case = {
            "description": doc.split('\n')[0],
            "effect": "none",  # Resources are read-only
            "uri": uri_pattern,
            "uri_variables": {},
            "expected_mime_type": "application/json",  # Default to JSON
            "content_validation": {
                "type": "json",
                "min_length": 1,
                "max_length": 100000,
                "schema": {"type": "object", "description": "Auto-generated basic schema - customize as needed"},
            },
        }

        # Detect URI template variables (e.g., {database}, {table}, {bucket})
        # FastMCP supports templated URIs when registered with add_resource_fn
        # The client expands templates with actual values, and FastMCP handles routing
        variables = re.findall(r'\{(\w+)\}', uri_pattern)
        for var in variables:
            # Substitute test values for common template variables
            if var == "bucket":
                # Use bucket name from QUILT_TEST_BUCKET environment variable (already loaded above)
                # Extract bucket name from s3:// URI
                bucket_name_var = (
                    test_bucket.replace("s3://", "").split("/")[0] if test_bucket.startswith("s3://") else test_bucket
                )
                test_case["uri_variables"][var] = bucket_name_var
            elif var == "database":
                # Use UserAthenaDatabase from stack
                test_case["uri_variables"][var] = athena_database
            elif var == "table":
                # Use a test table name
                test_case["uri_variables"][var] = "test_table"
            elif var == "name":
                # Use a test user name
                test_case["uri_variables"][var] = "test_user"
            elif var == "id":
                # Use a test workflow ID
                test_case["uri_variables"][var] = "test-workflow-001"
            else:
                # For unknown variables, mark as needing configuration
                test_case["uri_variables"][var] = f"CONFIGURE_{var.upper()}"

        # All resources return JSON by default
        # (FastMCP decorator-based resources all return JSON)

        test_config["test_resources"][uri_pattern] = test_case

    print(f"   Generated {len(test_config['test_resources'])} resource test cases")

    # Write YAML with nice formatting
    with open(output_file, 'w', encoding='utf-8') as f:
        yaml.dump(test_config, f, default_flow_style=False, sort_keys=False, allow_unicode=True, indent=2)


__all__ = [
    "generate_csv_output",
    "generate_json_output",
    "generate_test_yaml",
]
